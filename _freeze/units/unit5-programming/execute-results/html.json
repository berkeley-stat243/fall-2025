{
  "hash": "24ed7fc88b51d193112a3d809c2a1439",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Programming concepts\"\nauthor: \"Chris Paciorek\"\ndate: \"2024-09-13\"\nformat:\n  pdf:\n    documentclass: article\n    margin-left: 30mm\n    margin-right: 30mm\n    toc: true\n  html:\n    theme: cosmo\n    css: ../assets/styles.css\n    toc: true\n    code-copy: true\n    code-block-bg: true\n    code-block-border-left: \"#31BAE9\"\nexecute:\n  freeze: auto\nengine: knitr\nipynb-shell-interactivity: all\ncode-overflow: wrap\n---\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n# Overview\n\nThis unit covers a variety of programming concepts, illustrated in the\ncontext of Python and with comments about and connections to other languages. It also serves as a way to teach some advanced features of\nPython. In general the concepts are relevant in other languages, though other\nlanguages may implement things differently. One of my goals for the unit is for us\nto think about why things are the way they are in Python. I.e., what\nprinciples were used in creating the language and what choices were\nmade? While other languages use different principles and made different \nchoices, understanding what one language does in detail will be helpful when you\nare learning another language or choosing a language for a project.\n\n:::{.callout-note title=\"Quarto configuration details for this document\"}\nThis document  uses the `knitr` engine for rendering to be able to run chunks in multiple languages (Python and bash, as well as a few R chunks. It has the Quarto configuration `ipynb-shell-interactivity: all`, so that output from all Python code in a chunk will print in the rendered document; when done with the `knitr` engine, the output is interspersed with the code in the chunk rather than printed all at the end.\n:::\n\n# 1. Text manipulation, string processing and regular expressions (regex)\n\nText manipulations in Python have a number of things in common with UNIX, R,\nand Perl, as many of the ideas/software evolved from UNIX. When I use the term\n*string* here, I'll be referring to any sequence of characters that may\ninclude numbers, white space (including newlines), and special characters, usually stored as an object\nof the `str` class.\n\n## String processing and regular expressions in Python\n\nHere we'll see functionality for working with strings in Python, focusing on regular expressions with the `re` package. \nThis will augment our consideration of regular expressions in the shell, in particular\nby seeing how we can replace patterns in addition to finding them. \n\nThe `re` package provides Perl-style regular expressions, but it doesn't seem to support named character classes such as `[:digit:]`. Instead use classes such as `\\d` and `[0-9]`.\n\n### Finding patterns\n\nIn Python, you can apply a matching function and then query the result to get information about what was matched and where in the string. \n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntext = \"Here's my number: 919-543-3300.\"\nm = re.search(\"\\\\d+\", text)\nm\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<re.Match object; span=(18, 21), match='919'>\n```\n\n\n:::\n\n```{.python .cell-code}\nm.group()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'919'\n```\n\n\n:::\n\n```{.python .cell-code}\nm.start()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n18\n```\n\n\n:::\n\n```{.python .cell-code}\nm.end()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n21\n```\n\n\n:::\n\n```{.python .cell-code}\nm.span()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(18, 21)\n```\n\n\n:::\n:::\n\n\nNotice that that showed us only the first match.\n\nThe [discussion of special characters](#special-characters-in-python) explains why we need to provide `\\\\d` rather than `\\d`.\n\nWe can instead use `findall` to get all the matches.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nre.findall(\"\\\\d+\", text)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n['919', '543', '3300']\n```\n\n\n:::\n:::\n\n\n\nThis is equivalent to:\n\n\n::: {.cell}\n\n```{.python .cell-code}\npattern = re.compile(\"\\\\d+\")\nre.findall(pattern, text)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n['919', '543', '3300']\n```\n\n\n:::\n:::\n\n\nThe compile can be omitted and will be done implicitly, but is a good idea to do explicitly if you have a complex regex pattern that you will use repeatedly (e.g., on every line in a file). It is also a reminder that regular expressions is a separate language, which can be compiled into a program. The compilation results in an object that relies on finite state machines to match the pattern.\n\nTo ignore case, do the following:\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntext = \"That cat in the Hat\"\nre.findall(\"hat\", text, re.IGNORECASE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n['hat', 'Hat']\n```\n\n\n:::\n:::\n\n\nThere are several other regex flags (also called compilation flags) that can control the behavior of the matching engine in interesting ways (check out `re.VERBOSE` and `re.MULTILINE` for instance).\n\nWe can of course use list comprehension to work with multiple strings. But we need to be careful to check whether a match was found.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef return_group(pattern, txt):\n    m = re.search(pattern, txt)\n    if m:\n       return m.group()\n    else:\n       return None\n\ntext = [\"Here's my number: 919-543-3300.\", \"hi John, good to meet you\",\n        \"They bought 731 bananas\", \"Please call 1.919.554.3800\"]\n[return_group(\"\\\\d+\", str) for str in text]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n['919', None, '731', '1']\n```\n\n\n:::\n:::\n\n\nRecall that we can search for location-specific matches in relation to the start and end of a string.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntext = \"hats are all that are important to a hatter.\"\nre.findall(\"^hat\\\\w+\", text)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n['hats']\n```\n\n\n:::\n:::\n\n\nRecall that we can search based on repetitions (as already demonstrated with the `\\w+` just above).\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntext = \"Here's my number: 919-543-3300. They bought 731 hats. Please call 1.919.554.3800.\"\nre.findall(\"\\\\d{3}[-.]\\\\d{3}[-.]\\\\d{4}\", text)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n['919-543-3300', '919.554.3800']\n```\n\n\n:::\n:::\n\n\n\nAs another example, the phone number detection problem could have been done a bit more compactly (as well as more generally to allow for an initial \"1-\" or \"1.\") as:\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntext = \"Here's my number: 919-543-3300. They bought 731 bananas. Please call 1.919.554.3800.\"\nre.findall(\"((1[-.])?(\\\\d{3}[-.]){1,2}\\\\d{4})\", text)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[('919-543-3300', '', '543-'), ('1.919.554.3800', '1.', '554.')]\n```\n\n\n:::\n:::\n\n\nQuestion: the above regex would actually match something that is not a valid phone number. What can go wrong?\n\n\nWhen you are searching for all occurrences of a pattern in a large text object, it may be beneficial to use `finditer`:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nit = re.finditer(\"(http|ftp):\\\\/\\\\/\", text)  # http or ftp followed by ://\n\nfor match in it:\n    match.span()\n```\n:::\n\n\nThis method behaves lazily and returns an iterator that gives us one match at a time, and only scans for the next match when we ask for it. This is similar to the behavior we saw with `pandas.read_csv(chunksize = n)`\n\n### Manipulating and replacing patterns\n\nWe can replace matching substrings with `re.sub`.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntext = \"Here's my number: 919-543-3300.\"\nre.sub(\"\\\\d\", \"Z\", text   )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\"Here's my number: ZZZ-ZZZ-ZZZZ.\"\n```\n\n\n:::\n:::\n\n\nNext let's consider grouping using `()`. We'll see that the grouping operator also\ncontrols what is returned as the matched patterns.\n\nHere’s a basic example of using grouping via parentheses with the OR operator.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntext = \"At the site http://www.ibm.com. Some other text. ftp://ibm.com\"\nre.search(\"(http|ftp):\\\\/\\\\/\", text).group()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'http://'\n```\n\n\n:::\n:::\n\n\nHowever, if we want to find all the matches and try to use `findall`, we see that, when grouping operators are present, it returns only the \"captured\" groups, as discussed a bit in `help(re.findall)`,\nso we'd need to add an additional grouping operator to capture the full pattern when using `findall`:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nre.findall(\"(http|ftp):\\\\/\\\\/\", text)  \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n['http', 'ftp']\n```\n\n\n:::\n\n```{.python .cell-code}\nre.findall(\"((http|ftp):\\\\/\\\\/)\", text) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[('http://', 'http'), ('ftp://', 'ftp')]\n```\n\n\n:::\n:::\n\n\nIf we only wanted to full pattern without capturing the inner group, we can use some additional syntax, `?:`, for \"non-capturing\" groups:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nre.findall(\"((?:http|ftp):\\\\/\\\\/)\", text) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n['http://', 'ftp://']\n```\n\n\n:::\n:::\n\n\n\nGroups are also used when we need to reference back to a detected pattern when doing a replacement. This is why they are sometimes referred to as \"capturing groups\". For example, here we’ll find any numbers and add underscores before and after them:\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntext = \"Here's my number: 919-543-3300. They bought 731 bananas. Please call 919.554.3800.\"\nre.sub(\"([0-9]+)\", \"_\\\\1_\", text)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\"Here's my number: _919_-_543_-_3300_. They bought _731_ bananas. Please call _919_._554_._3800_.\"\n```\n\n\n:::\n:::\n\n\nHere we’ll remove commas not used as field separators.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntext = '\"H4NY07011\",\"ACKERMAN, GARY L.\",\"H\",\"$13,242\",,,'\nre.sub(\"([^\\\",]),\", \"\\\\1\", text)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'\"H4NY07011\",\"ACKERMAN GARY L.\",\"H\",\"$13242\",,,'\n```\n\n\n:::\n:::\n\n\nHow does that work? Consider that `[^\\\",]` matches a character that is not a quote and not a comma. The regex is such a character followed by a comma, with the matched character saved in `\\\\1` because of the grouping operator.\n\n:::{.callout-tip title=\"Challenge\"}\nInstead of removing the commas to remove the ambiguity, how would you convert the comma delimiters to pipe (`|`) delimiters (since the pipe is rarely used in text)?\n:::\n\nExtending the use of `\\\\1`, we can refer to multiple captured groups:\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntext = \"Here's my number: 919-543-3300. They bought 731 bananas. Please call 919.554.3800.\"\nre.sub(\"([0-9]{3})[-\\.]([0-9]{3})[-\\.]([0-9]{4})\", \"area code \\\\1, number \\\\2-\\\\3\", text)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\"Here's my number: area code 919, number 543-3300. They bought 731 bananas. Please call area code 919, number 554-3800.\"\n```\n\n\n:::\n:::\n\n\n:::{.callout-note title=\"Additional regex functionality\"}\n\nRegex extensions that we won't discuss further here include:\n\n - Groups can also be given names, instead of having to refer to them by their numbers.\n - We can have `sub` call a \"callback\" function to do the replacement. The function will be invoked on each match with the argument being equivalent to the result of `re.search`.\n  - We can reference previously captured groups within a pattern using the same `\\\\1`-style syntax (as opposed to when doing replacement as seen above).\n:::\n\n:::{.callout-tip title=\"Challenge\"}\n\nSuppose a text string has dates in the form \"Aug-3\", \"May-9\", etc. and I want them in the form \"3 Aug\", \"9 May\", etc. How would I do this regex?\n\n:::\n\n### Greedy matching\n\nFinally, let's consider where a match ends when there is ambiguity.\n\nAs a simple example consider that if we try this search, we match as many digits as possible, rather than returning the first \"9\" as satisfying the request for \"one or more\" digits.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntext = \"See the 998 balloons.\"\nre.findall(\"\\\\d+\", text)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n['998']\n```\n\n\n:::\n:::\n\n\nThat behavior is called *greedy* matching, and it's the default. That example also shows why it\nis the default. What would happen if it were not the default?\n\nHowever, sometimes greedy matching doesn't get us what we want.\n\nConsider this attempt to remove multiple html tags from a string.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntext = \"Do an internship <b> in place </b> of <b> one </b> course.\"\nre.sub(\"<.*>\", \"\", text)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'Do an internship  course.'\n```\n\n\n:::\n:::\n\n\nNotice what happens because of greedy matching.\n\nOne way to avoid greedy matching is to use a `?` after the repetition specifier.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nre.sub(\"<.*?>\", \"\", text)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'Do an internship  in place  of  one  course.'\n```\n\n\n:::\n:::\n\n\nHowever, that syntax is a bit frustrating because `?` is also used to indicate\n0 or 1 repetitions, making the regex a bit hard to read/understand.\n\n:::{.callout-tip title=\"Challenge\"}\n\nSuppose I want to strip out HTML tags but without using the `?` to avoid\ngreedy matching. How can I be more careful in constructing my regex?\n:::\n\n## Special characters in Python\n\nRecall that when characters are used for special purposes, we need to\n'escape' them if we want them interpreted as the actual (*literal*) character. In what\nfollows, I show this in Python, but similar manipulations are sometimes\nneeded in the shell and in R.\n\nThis can get particularly confusing in Python as the backslash is also used\nto input special characters such as newline (`\\n`) or tab (`\\t`).\nApparently there was some change in handling *escape sequences* as of Python 3.12. We now need to do this for the regex `\\d`:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nre.search(\"\\\\d+\", \"a93b\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<re.Match object; span=(1, 3), match='93'>\n```\n\n\n:::\n:::\n\n\nIn Python 3.11, it was fine to use `\\d`, but now we need `\\\\d`, because Python now tries to interpret `\\d` as a special character\n(like `\\n`, but `\\d` doesn't exist) and doesn't pass it directly along as regex syntax.\n\nHere are some examples of using special characters.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntmp = \"Harry said, \\\"Hi\\\"\"\nprint(tmp)   # This prints out without a newline -- this is hard to show in rendered doc.\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nHarry said, \"Hi\"\n```\n\n\n:::\n\n```{.python .cell-code}\ntmp = \"Harry said, \\\"Hi\\\".\\n\"\nprint(tmp)   # This prints out with the newline -- hard to show in rendered doc.\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nHarry said, \"Hi\".\n```\n\n\n:::\n\n```{.python .cell-code}\ntmp = [\"azar\", \"foo\", \"hello\\tthere\\n\"]\nprint(tmp[2])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nhello\tthere\n```\n\n\n:::\n\n```{.python .cell-code}\nre.search(\"[\\tZ]\", tmp[2])   # Search for a tab or a 'Z'.\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<re.Match object; span=(5, 6), match='\\t'>\n```\n\n\n:::\n:::\n\n\nHere are some examples of using various special characters in regex syntax.\nTo use a special character as a regular character, we need to escape it\n(which in Python 3.12 involves two backslashes, as discussed above):\n\n\n::: {.cell}\n\n```{.python .cell-code}\n## Search for characters that are not 'z'\n## (using ^ as regular expression syntax)\nre.search(\"[^z]\", \"zero\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<re.Match object; span=(1, 2), match='e'>\n```\n\n\n:::\n\n```{.python .cell-code}\n## Show results for various input strings:\nfor st in [\"a^2\", \"93\", \"zzz\", \"zit\", \"azar\"]:\n    print(st + \":\\t\", re.search(\"[^z]\", st))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\na^2:\t <re.Match object; span=(0, 1), match='a'>\n93:\t <re.Match object; span=(0, 1), match='9'>\nzzz:\t None\nzit:\t <re.Match object; span=(1, 2), match='i'>\nazar:\t <re.Match object; span=(0, 1), match='a'>\n```\n\n\n:::\n\n```{.python .cell-code}\n    \n\n## Search for either a '^' (as a regular character) or a 'z':\nfor st in [\"a^2\", \"93\", \"zzz\", \"zit\", \"azar\"]:\n    print(st + \":\\t\", re.search(\"[\\\\^z]\", st))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\na^2:\t <re.Match object; span=(1, 2), match='^'>\n93:\t None\nzzz:\t <re.Match object; span=(0, 1), match='z'>\nzit:\t <re.Match object; span=(0, 1), match='z'>\nazar:\t <re.Match object; span=(1, 2), match='z'>\n```\n\n\n:::\n\n```{.python .cell-code}\n    \n\n## Search for exactly three characters\n## (using . as regular expression syntax)\nfor st in [\"abc\", \"1234\", \"def\"]:\n    print(st + \":\\t\", re.search(\"^.{3}$\", st))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nabc:\t <re.Match object; span=(0, 3), match='abc'>\n1234:\t None\ndef:\t <re.Match object; span=(0, 3), match='def'>\n```\n\n\n:::\n\n```{.python .cell-code}\n    \n\n## Search for a period (as a regular character)\nfor st in [\"3.9\", \"27\", \"4.2\"]:\n    print(st + \":\\t\", re.search(\"\\\\.\", st)) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n3.9:\t <re.Match object; span=(1, 2), match='.'>\n27:\t None\n4.2:\t <re.Match object; span=(1, 2), match='.'>\n```\n\n\n:::\n:::\n\n\n:::{.callout-tip title=\"Challenge\"}\nExplain why we use a single backslash to get a newline and double backslash to write out a Windows path in the examples here:\n\n\n::: {.cell}\n\n```{.python .cell-code}\n## Suppose we want to use a \\ in our string:\nprint(\"hello\\nagain\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nhello\nagain\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(\"hello\\\\nagain\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nhello\\nagain\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(\"My Windows path is: C:\\\\Users\\\\nadal.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMy Windows path is: C:\\Users\\nadal.\n```\n\n\n:::\n:::\n\n\nAnother way to achieve this effect if your string does not contain any special characters is to prefix your string literal with an r for \"raw\":\n\n\n::: {.cell}\n\n```{.python .cell-code}\nprint(r\"My Windows path is: C:\\Users\\nadal.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMy Windows path is: C:\\Users\\nadal.\n```\n\n\n:::\n:::\n\n:::\n\nOn a more involved note, searching for an actual backslash gets even more\ncomplicated (you can search online for [\"backslash plague\"](https://docs.python.org/3/howto/regex.html#the-backslash-plague) or \"backslash hell\"), because we need to pass two backslashes as the regular expression, so that a literal backslash is searched for. However, to pass two backslashes, we need to escape each of them with a backslash so Python doesn't treat each backslash as part of a special character. So that's four backslashes to search for a single backslash! Yikes. One rule of\nthumb is just to keep entering backslashes until things work!\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n## Use and search for an actual backslash\ntmp = \"something \\\\ other\\n\"\nprint(tmp) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nsomething \\ other\n```\n\n\n:::\n\n```{.python .cell-code}\nre.search(\"\\\\\\\\\", tmp)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<re.Match object; span=(10, 11), match='\\\\'>\n```\n\n\n:::\n\n```{.python .cell-code}\ntry:\n    re.search(\"\\\\\", tmp)\nexcept Exception as error:\n    print(error)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nbad escape (end of pattern) at position 0\n```\n\n\n:::\n:::\n\n\nAgain here you can use \"raw\" strings, at the price of losing the ability to use any special characters:\n\n\n::: {.cell}\n\n```{.python .cell-code}\n## Search for an actual backslash\nre.search(r\"\\\\\", tmp)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<re.Match object; span=(10, 11), match='\\\\'>\n```\n\n\n:::\n:::\n\n\nThe use of the raw string `r\"\\\\\"` tells Python to treat this string literal without any escaping, but that does not apply to the regex engine (or else we would have used a single backslash), so we do need the second backslash. So yes. This can be quite confusing.\n\n\n:::{.callout-warning title=\"Pasting quotation marks\"}\nBe careful when cutting and pasting from documents that are not text\nfiles as you may paste in something that looks like a single or double\nquote, but which Python cannot interpret as a quote because it's some other\nASCII (or Unicode) quote character. If you paste in a \" from PDF, it will not be\ninterpreted as a standard Python double quote mark.\n:::\n\nSimilar things come up in the shell and in R, but in the shell you\noften don't need as many backslashes. E.g. you could do this to look for a\nliteral backslash character.\n\n\n::: {.cell}\n\n```{.bash .cell-code}\necho \"hello\" > file.txt\necho \"with a \\ there\" >> file.txt\ngrep '\\\\' file.txt\n```\n:::\n\n```\nwith a \\ there\n```\n\n\n\n# 2. Interacting with the operating system and external code and configuring Python \n\n## Interacting with the operating system\n\nScripting languages allow one to interact with the operating system in various ways.\nMost allow you to call out to the shell to run arbitrary shell code and save results within your session.\n\nI'll assume everyone knows about the following functions/functionality for interacting with the filesystem and file\nin Python: `os.getcwd`, `os.chdir`, `import`, `pickle.dump`, `pickle.load`\n\nAlso in IPython there is additional functionality/syntax.\n\nHere are a variety of tools for interacting with the operating system:\n\n-   To run UNIX commands from within Python, use `subprocess.run()`, as follows,\n    noting that we can save the result of a system call to an Python object:\n\n\n    ::: {.cell}\n    \n    ```{.python .cell-code}\n    import subprocess, io\n    subprocess.run([\"ls\", \"-al\"])   ## results apparently not shown when compiled...\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    CompletedProcess(args=['ls', '-al'], returncode=0)\n    ```\n    \n    \n    :::\n    \n    ```{.python .cell-code}\n    files = subprocess.run([\"ls\", \"-al\"], capture_output = True)\n    files.stdout\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    b'total 1467\\ndrwxr-sr-x 12 paciorek scfstaff      61 Sep 15 07:44 .\\ndrwxr-sr-x 14 paciorek scfstaff      43 Sep 12 08:28 ..\\n-rw-r--r--  1 paciorek scfstaff    2279 Aug  6 12:23 badCode.R\\n-rw-r--r--  1 paciorek scfstaff     803 Aug  6 12:23 book.yml\\n-rw-r--r--  1 paciorek scfstaff  117142 Aug  6 12:23 chatgpt-regex-numbers.png\\n-rw-r--r--  1 paciorek scfstaff   41130 Sep  4 08:07 cpds.csv\\n-rw-r--r--  1 paciorek scfstaff     236 Aug  6 12:23 dummy.py\\n-rw-r--r--  1 paciorek scfstaff  175396 Aug  6 12:23 exampleGraphic.png\\n-rw-r--r--  1 paciorek scfstaff 1036183 Aug  6 12:23 file_nonascii.txt\\n-rw-r--r--  1 paciorek scfstaff   14610 Aug  6 12:23 gauss-seidel.png\\n-rw-r--r--  1 paciorek scfstaff      94 Aug 12 12:10 .github-access-token.txt\\n-rw-r--r--  1 paciorek scfstaff      41 Aug 12 12:00 .github-access-token.txt~\\n-rw-r--r--  1 paciorek scfstaff    4038 Aug  6 12:23 goodCode.R\\ndrwxr-sr-x  2 paciorek scfstaff       9 Aug  6 12:23 graphics_files\\n-rw-r--r--  1 paciorek scfstaff   20260 Aug  6 12:23 graph.png\\n-rw-r--r--  1 paciorek scfstaff    2464 Aug  6 12:23 linked-list.png\\n-rw-r--r--  1 paciorek scfstaff      98 Aug  6 12:23 local.py\\n-rw-r--r--  1 paciorek scfstaff      79 Aug  6 12:23 mymod.py\\ndrwxr-sr-x  4 paciorek scfstaff       7 Aug  6 13:38 mypkg\\n-rw-r--r--  1 paciorek scfstaff   27757 Aug  6 12:23 nelder-mead.png\\n-rw-r--r--  1 paciorek scfstaff   14575 Aug 12 11:49 newest.json\\n-rw-r--r--  1 paciorek scfstaff   22698 Aug 12 11:41 newest.xml\\n-rw-r--r--  1 paciorek scfstaff   63998 Aug  6 12:23 normalized_example.png\\ndrwxr-sr-x  2 paciorek scfstaff       8 Sep  2 09:55 __pycache__\\ndrwxr-xr-x  3 paciorek scfstaff       6 Sep  2 09:53 .pytest_cache\\n-rw-r--r--  1 paciorek scfstaff    2418 Sep  8 09:07 regex.qmd\\n-rw-------  1 paciorek scfstaff     118 Aug  6 13:44 .Rhistory\\ndrwxr-sr-x  3 paciorek scfstaff       5 Sep  2 09:52 .ruff_cache\\n-rw-r--r--  1 paciorek scfstaff     573 Aug  6 12:23 run_no_break.py\\n-rw-r--r--  1 paciorek scfstaff     591 Aug  6 12:23 run_with_break.py\\n-rw-r--r--  1 paciorek scfstaff   15667 Aug  6 12:23 steep-descent.png\\n-rw-r--r--  1 paciorek scfstaff     417 Aug 29 07:52 test_dummy.py\\n-rw-r--r--  1 paciorek scfstaff      74 Sep  2 11:21 test.py\\n-rw-r--r--  1 paciorek scfstaff      74 Sep  2 11:21 test-save.py\\n-rw-r--r--  1 paciorek scfstaff     111 Aug  6 12:23 test_scope.py\\n-rw-r--r--  1 paciorek scfstaff       0 Aug 12 14:50 test.sh\\n-rw-r--r--  1 paciorek scfstaff      74 Sep  2 10:03 test-unlinted.py\\n-rw-r--r--  1 paciorek scfstaff      10 Sep  3 08:56 tmp2.txt\\n-rw-r--r--  1 paciorek scfstaff       4 Sep  3 08:56 tmp.txt\\n-rw-r--r--  1 paciorek scfstaff       4 Aug 13 08:51 tmp.txt~\\n-rw-r--r--  1 paciorek scfstaff    9357 Aug  6 12:23 tree.png\\ndrwxr-sr-x  4 paciorek scfstaff       4 Aug 11 12:39 unit10-linalg_cache\\n-rw-r--r--  1 paciorek scfstaff   88399 Aug 26 10:47 unit10-linalg.qmd\\n-rw-r--r--  1 paciorek scfstaff  111351 Aug 26 10:47 unit11-optim.qmd\\n-rw-r--r--  1 paciorek scfstaff   16296 Aug 26 10:47 unit12-graphics.qmd\\n-rw-r--r--  1 paciorek scfstaff   11498 Aug 26 11:36 unit1-intro.qmd\\n-rw-r--r--  1 paciorek scfstaff   60648 Sep  3 07:51 unit2-dataTech.qmd\\n-rw-r--r--  1 paciorek scfstaff   19687 Sep  2 11:16 unit3-bash.qmd\\n-rw-r--r--  1 paciorek scfstaff   49636 Sep  2 11:05 unit4-goodPractices.qmd\\ndrwxr-sr-x  4 paciorek scfstaff       4 Aug  6 14:55 unit5-programming_cache\\ndrwxr-sr-x  4 paciorek scfstaff       4 Sep 10 14:25 unit5-programming_files\\n-rw-r--r--  1 paciorek scfstaff  147792 Sep 11 11:36 unit5-programming.qmd\\n-rw-r--r--  1 paciorek scfstaff  147793 Sep 15 07:44 unit5-programming.rmarkdown\\ndrwxr-sr-x  4 paciorek scfstaff       4 Aug 11 12:35 unit6-parallel_cache\\n-rw-r--r--  1 paciorek scfstaff   55948 Aug 26 10:47 unit6-parallel.qmd\\ndrwxr-sr-x  4 paciorek scfstaff       4 Aug 13 09:00 unit7-bigData_cache\\n-rw-r--r--  1 paciorek scfstaff   60874 Aug 26 10:47 unit7-bigData.qmd\\n-rw-r--r--  1 paciorek scfstaff   31946 Aug 26 10:47 unit8-numbers.qmd\\n-rw-r--r--  1 paciorek scfstaff   47693 Aug 26 10:47 unit9-sim.qmd\\n-rw-r--r--  1 paciorek scfstaff     142 Aug  6 12:23 vec_orig.py\\n-rw-r--r--  1 paciorek scfstaff     142 Aug 13 08:58 vec.py\\n'\n    ```\n    \n    \n    :::\n    \n    ```{.python .cell-code}\n    with io.BytesIO(files.stdout) as stream:  # create a file-like object\n         content = stream.readlines()\n    content[2:4]\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    [b'drwxr-sr-x 14 paciorek scfstaff      43 Sep 12 08:28 ..\\n', b'-rw-r--r--  1 paciorek scfstaff    2279 Aug  6 12:23 badCode.R\\n']\n    ```\n    \n    \n    :::\n    :::\n\n\n-   There are also a bunch of functions that will do specific queries of\n    the filesystem, including\n\n\n    ::: {.cell}\n    \n    ```{.python .cell-code}\n    os.path.exists(\"unit2-dataTech.qmd\")\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    True\n    ```\n    \n    \n    :::\n    \n    ```{.python .cell-code}\n    os.listdir(\"../data\")\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    ['hivSequ.csv', 'cpds.csv', 'precipData.txt', 'stackoverflow-2021.db', 'coop.txt.gz', 'airline.csv', 'precip.txt', 'test.py', '0', 'test.r', 'co2_annmean_mlo.csv', 'airline.parquet', 'RTADataSub.csv']\n    ```\n    \n    \n    :::\n    :::\n\n\n-   There are some tools for dealing with differences between operating\n    systems. `os.path.join` is a nice example:\n\n\n    ::: {.cell}\n    \n    ```{.python .cell-code}\n    os.listdir(os.path.join(\"..\", \"data\"))\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    ['hivSequ.csv', 'cpds.csv', 'precipData.txt', 'stackoverflow-2021.db', 'coop.txt.gz', 'airline.csv', 'precip.txt', 'test.py', '0', 'test.r', 'co2_annmean_mlo.csv', 'airline.parquet', 'RTADataSub.csv']\n    ```\n    \n    \n    :::\n    :::\n\n\n    It's best if you can to write your code, as shown here with `os.path.join`, in a way that is *agnostic* to the underlying operating system (i.e., that works regardless of the operating system).\n\n-   To get some info on the system you're running on:\n\n\n    ::: {.cell}\n    \n    ```{.python .cell-code}\n    import platform\n    platform.system()\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    'Linux'\n    ```\n    \n    \n    :::\n    \n    ```{.python .cell-code}\n    os.uname()\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    posix.uname_result(sysname='Linux', nodename='smeagol', release='6.8.0-64-generic', version='#67-Ubuntu SMP PREEMPT_DYNAMIC Sun Jun 15 20:23:31 UTC 2025', machine='x86_64')\n    ```\n    \n    \n    :::\n    \n    ```{.python .cell-code}\n    platform.python_version()\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    '3.13.2'\n    ```\n    \n    \n    :::\n    :::\n\n\n-   To retrieve environment variables:\n\n    ::: {.cell}\n    \n    ```{.python .cell-code}\n    os.environ['PATH']\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    '/system/linux/miniforge-3.13/bin:/system/linux/miniforge-3.13/condabin:/system/linux/miniforge-3.13/condabin:/system/linux/miniforge-3.13/bin:/system/linux/miniforge-3.13/condabin:/system/linux/miniforge-3.13/bin:/system/linux/miniforge-3.13/condabin:/system/linux/miniforge-3.13/bin:/usr/local/linux/julia-1.10.4/bin:/usr/local/linux/miniforge-3.13/bin:/accounts/vis/paciorek/bin:/usr/local/linux/bin:/usr/local/bin:/usr/bin:/usr/sbin:/usr/lib/rstudio-server/bin:/accounts/gen/vis/paciorek/.local/bin'\n    ```\n    \n    \n    :::\n    :::\n\n\n-   You can have an Python script act as a shell script (like running a bash\n    shell script) as follows. \n\n    1.  Write your Python code in a text file, say `example.py`\n    2.  As the first line of the file, include `#!/usr/bin/python`\n        (like `#!/bin/bash` in a bash shell file, as seen in Unit 2) or\n        for more portability across machines, include\n        `#!/usr/bin/env python`.\n    3.  Make the Python code file executable with `chmod`:\n        `chmod ugo+x example.py`.\n    4.  Run the script from the command line: `./example.py`\n\n    If you want to pass arguments into your script, you can do so with the\n    `argparse` package.\n\n\n    ::: {.cell}\n    \n    ```{.python .cell-code}\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument('-y', '--year', default=2002,\n                        help='year to download')\n    parser.add_argument('-m', '--month', default=None,\n                        help='month to download')\n    args = parse.parse_args()\n    args.year\n    year = int(args.year)\n    ```\n    :::\n\n\n    Now we can run it as follows in the shell:\n\n\n    ::: {.cell}\n    \n    ```{.bash .cell-code}\n    ./example.py 2004 January\n    ```\n    :::\n\n\n-   Use `Ctrl-C` to interrupt execution. This will generally back out\n    gracefully, returning you to a state as if the command had not been\n    started. Note that if Python is exceeding the amount of memory available, there can\n    be a long delay. This can be frustrating, particularly since a\n    primary reason you would want to interrupt is when Python runs out of\n    memory.\n\n\n## Interacting with external code\n\nScripting languages such as R, Python, and Julia allow you to call out to \"external code\",\nwhich often means C or C++ (but also Fortran, Java and other languages). \n\nCalling out to external code is particularly important in languages like R and Python that are often much slower\nthan compiled code and less important in a fast language like Julia (which uses Just-In-Time compilation -- more on that later).\n\nIn fact, the predecessor language to R,\nwhich was called 'S' was developed specifically (at AT&T's Bell Labs in the 1970s and 1980s) as an interactive\nwrapper around Fortran, the numerical programming language most commonly used at the time (and still widely relied on today in various legacy codes). \n\nIn Python, one can [directly call out to C or C++ code](https://docs.python.org/3/extending/extending.html) or one can use *Cython* to interact with C. With Cython, one can:\n\n  - Have Cython automatically translate Python code to C, if you provide type definitions for your variables.\n  - Define C functions that can be called from your Python code.\n\nIn R, one can call directly out to C or C++ code using *.Call* or one can use the [Rcpp package](https://adv-r.hadley.nz/rcpp.html). *Rcpp* is specifically designed to be able to write C++ code that feels somewhat like writing R code and where it is very easy to pass data between R and C++. \n\n# 3. Modules and packages\n\nScripting languages that become popular generally have an extensive collection\nof add-on packages available online (the causal relationship of the popularity and\nthe extensive add-on packages goes in both directions). \n\nA big part of Python's popularity is indeed the extensive collection of add-on\npackages on [PyPI](https://pypi.org) (and GitHub and elsewhere) and\nvia `Conda` that provide\nmuch of Python's functionality (including core numerical capabilities via `numpy` and `scipy`).\n\nTo make use of a package it needs to be\ninstalled on your system (using `pip install` or `conda install`) *once* and\nloaded into Python (using the `import` statement) *every time you start a new session*.\n\nSome modules are *installed* by default with Python (e.g., `os` and `re`),\nbut all need to be loaded by the user in a given Python session.\n\n## Modules\n\nA *module* is a collection of related code in a file with the extension `.py`.\nThe code can include functions, classes, and variables, as well as\nrunnable code. To access the objects in the module, you need to import the module.\n\nHere we'll create `mymod.py` from the shell, but of course usually one would create it in an editor.\n\n\n::: {.cell}\n\n```{.bash .cell-code}\ncat << EOF > mymod.py\nx = 7\nrange = 3\ndef myfun(x):\n    print(\"The arg is: \", str(x), \".\", sep = '')\nEOF\n```\n:::\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport mymod\nprint(mymod.x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n7\n```\n\n\n:::\n\n```{.python .cell-code}\nmymod.myfun(7)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nThe arg is: 7.\n```\n\n\n:::\n:::\n\n\n## The import statement\n\nThe import statement allows one to get access to code in a module.\nImportantly it associates the names of the objects in the module with\na name accessible in the scope in which it was imported (i.e., the current context). The mapping of\nnames (references) to objects is called a *namespace*. We discuss\n[scopes and namespaces](#namespaces-and-scopes) in more detail later.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndel mymod\ntry:           # Check if `mymod` is in scope.\n    mymod.x\nexcept Exception as error:\n    print(error)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nname 'mymod' is not defined\n```\n\n\n:::\n\n```{.python .cell-code}\ny = 3\n\nimport mymod\nmymod         # This is essentially a dictionary in the current (global) scope.\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<module 'mymod' from '/accounts/vis/paciorek/teaching/243fall25/fall-2025/units/mymod.py'>\n```\n\n\n:::\n\n```{.python .cell-code}\nx             # This is not a name in the current (global) scope.\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNameError: name 'x' is not defined\n```\n\n\n:::\n\n```{.python .cell-code}\nrange         # This is a builtin, not from the module.\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<class 'range'>\n```\n\n\n:::\n\n```{.python .cell-code}\nmymod.x\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n7\n```\n\n\n:::\n\n```{.python .cell-code}\ndir(mymod)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'myfun', 'range', 'x']\n```\n\n\n:::\n\n```{.python .cell-code}\nmymod.x\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n7\n```\n\n\n:::\n\n```{.python .cell-code}\nmymod.range\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n3\n```\n\n\n:::\n:::\n\n\nSo `y` and `mymod` are in the global namespace and `range` and `x` are in the module namespace of `mymod`. You can access the built-in `range` function from the global namespace but it turns out it's actually in the built-ins scope (more later).\n\nNote the usefulness of distinguishing the objects in a module from those in the global namespace.\nWe'll discuss this more in a bit.\n\nThat said, we can make an object defined in a module directly accessible in the current scope (adding it to the global namespace in this example) at which point it is distinct from the object in the module:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom mymod import x\nx   # now part of global namespace\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n7\n```\n\n\n:::\n\n```{.python .cell-code}\ndir()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n['__annotations__', '__builtins__', '__doc__', '__loader__', '__name__', '__package__', '__spec__', 'content', 'files', 'io', 'it', 'm', 'math', 'mymod', 'os', 'pattern', 'platform', 'r', 're', 'return_group', 'st', 'stream', 'subprocess', 'sys', 'text', 'time', 'tmp', 'x', 'y']\n```\n\n\n:::\n\n```{.python .cell-code}\nmymod.x = 5\nx = 3\nmymod.x\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n5\n```\n\n\n:::\n\n```{.python .cell-code}\nx\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n3\n```\n\n\n:::\n:::\n\n\nBut in general we wouldn't want to use `from` to import objects in that fashion because we could introduce name *conflicts* and we reduce modularity.\n\nThat said, it can be tedious to always have to type the module name (and in many cases there are multiple submodule names you'd also need to type).\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport mymod as m\nm.x\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n5\n```\n\n\n:::\n:::\n\n\n## Packages\n\nA package is a directory containing one or more modules and with a file\nnamed `__init__.py` that is called when a package is imported and\nserves to initialize the package.\n\nLet's create a basic package.\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nmkdir mypkg\n\ncat << EOF > mypkg/__init__.py\n## Make objects from mymod.py available as mypkg.foo rather than mypkg.mymod.foo.\n## The \".\" is a \"relative\" import that means find \"mymod\" here in this directory.\nfrom .mymod import *\n\nprint(\"Welcome to my package.\")\nEOF\n\ncat << EOF > mypkg/mymod.py\nx = 7\n\ndef myfun(val):\n    print(f\"Converting {val} to integer: {int(val)}.\")\nEOF\n```\n:::\n\n\nNote that if there were other modules, we could have imported from those as well.\n\nNow we can use the objects from the module without having to know\nthat it was in a particular module (because of how `__init__.py` was set up).\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport mypkg\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nWelcome to my package.\n```\n\n\n:::\n\n```{.python .cell-code}\nmypkg.x\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n7\n```\n\n\n:::\n\n```{.python .cell-code}\nmypkg.myfun(7.3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConverting 7.3 to integer: 7.\n```\n\n\n:::\n:::\n\n\nNote, one can set `__all__` in an `__init__.py` to define what is imported,\nwhich makes clear what is publicly available and hides what is considered\ninternal.\n\n### Internal/private objects\n\nWe could add another module that the main module uses but that is not intended\nfor direct use by the user. Here we name the function to start with `_` following\nthe convention that this indicates a private/internal function.\n\n\n::: {.cell}\n\n```{.bash .cell-code}\ncat << EOF > mypkg/auxil.py\n\ndef _helper(val):\n    return val + 10\nEOF\n\n\ncat << EOF >> mypkg/mymod.py\n\nfrom .auxil import _helper\n\ndef myfun10(val):\n    print(f\"Converting {val} to integer plus 10: {int(_helper(val))}.\")\nEOF\n```\n:::\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndel mypkg\nimport mypkg \nmypkg.myfun10(7.3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConverting 7.3 to integer plus 10: 17.\n```\n\n\n:::\n\n```{.python .cell-code}\ndel mypkg\n```\n:::\n\n\n\n### Subpackages\n\nPackages can also have modules in nested directories, achieving additional modularity\nvia *subpackages*.\nA package can automatically import the subpackages via the main `__init__.py`\nor require the user to import them manually, e.g., `import mypkg.mysubpkg`.\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nmkdir mypkg/mysubpkg\n\ncat << EOF > mypkg/mysubpkg/__init__.py\nfrom .values import *\nprint(\"Welcome to my package's subpackage.\")\nEOF\n\ncat << EOF > mypkg/mysubpkg/values.py\nx = 999\nb = 7\nd = 9\nEOF\n```\n:::\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport mypkg.mysubpkg     ## Note that __init__.py is invoked\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nWelcome to my package's subpackage.\n```\n\n\n:::\n\n```{.python .cell-code}\nmypkg.mysubpkg.b\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n7\n```\n\n\n:::\n\n```{.python .cell-code}\nmypkg.x\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n7\n```\n\n\n:::\n:::\n\n\nNote that a given `__init__.py` is invoked when importing anything nested within the\ndirectory containing the `__init__.py`; in the case above the `__init__.py` from `mypkg`\nis invoked, though for some reason the \"Welcome to my package.\" output is not showing \nup in this rendered document.\n\nIf we wanted to automatically import the subpackage we would add\n`from . import mysubpkg` to `mypkg/__init__.py`, which uses [relative imports](https://docs.python.org/3/reference/import.html). The alternative \"absolute\" import would be `import mypkg.mysubpkg`, which finds `mypkg` using `sys.path` (which specifies a set of paths of where to look).\n\n\nOne would generally not import the items from `mysubpkg` directly into the `mypkg` namespace\nbut there may be cases one would do something like this. For example `numpy.linspace` is actually\nfound in `numpy/core/function_base.py`, but we don't need to refer to `numpy.core.linspace`\nbecause of how `numpy` structures the `import` statements in `__init__.py`. In contrast, the linear algebra\nfunctions are available via the subpackage namespace as `numpy.linalg.<function_name>`.\n\nTake a look at `dir(numpy)`, `dir(numpy.linalg)`, and `dir(numpy.core)` to get a better sense for this in a real package.\n\n## Installing packages\n\nIf a package is on PyPI or available through Conda but not on your system, you can install it\neasily (usually). You don't need root permission on a machine to install\na package, though you may need to use `pip install --user` or set up a new Conda environment.\n\nPackages often depend on other packages. In general, if one package depends on another,\npip or conda will generally install the dependency automatically.\n\nOne advantage of Conda is that it can also install non-Python packages on which a Python\npackage depends, whereas with pip you sometimes need to install a system package to\nsatisfy a dependency.\n\n:::{.callout-note title=\"Using Mamba/libmamba and the Conda-forge channel\"}\nIt's not uncommon to run into a case where conda has trouble installing a package\nbecause of version inconsistencies amongst the dependencies. `mamba` is a drop-in\nreplacement for `conda` and often does a better job of this \"dependency resolution\".\n[We use `mamba` by default on the SCF](https://statistics.berkeley.edu/computing/software/install).\nIn recent versions of Conda, you can also use the Mamba's dependency resolver when running `conda` commands by running\n`conda config --set solver libmamba`, which puts `solver: libmamba` in your `.condarc` file.\nIt's also generally recommended to use the `conda-forge` *channel* (i.e., location) when installing packages with Conda\n(this is done automatically when using `mamba`). `conda-forge` provides a wide variety of up-to-date packages, maintained by the community.\n:::\n\n\n### Making your package installable (optional)\n\nIt's pretty easy to configure your package so that it can be built and installed via `pip`. See the structure of\n[this example repository](https://github.com/fperez/mytoy). In fact, one can install the package with only\neither `setup.py` or `pyproj.toml`, but the other files listed here are recommended:\n\n- `pyproj.toml` (or `pyproject.toml`): this is a configuration file used by packaging tools. In the `mytoy` example it specifies to use `setuptools` to build and install the package.\n- `setup.py`: this is run when the package is built and installed when using `setuptools`. In the example, it simply runs `setuptools.setup()`. With recent versions of\n`setuptools`, you don't actually need this so long as you have the `pyproj.toml` file.\n- `setup.cfg`: provides metadata about the package when using `setuptools`.\n- `environment.yml`: provides information about the full environment in which your package should be used (including examples, documentation, etc.). For projects using `setuptools`, a minimal list of dependencies needed for installation and use of the package can instead be included in the `install_requires` option of `setup.cfg`.\n- `LICENSE`: specifies the license for your package giving the terms under which others can use it.\n\nThe `postBuild` file is  a completely optional file only needed if you want to use the package with a MyBinder environment.\n\nAt the [numpy GitHub repository](https://github.com/numpy/numpy), by looking in  `pyproject.toml`, you can see that `numpy` is build and installed using a system called *Meson*,\nwhile at the [Jupyter GitHub repository](https://github.com/jupyter/jupyter) you can see that the `jupyter` package is built and installed using `setuptools`.\n\n*Building* a package usually refers to compiling source code but for a Python package that just has Python code, nothing needs to be compiled. *Installing* a package means putting the built package into a location on your computer where packages are installed.\n\nYou can also make your package public on PyPI or through Conda, but that is not something we'll cover here.\n\n### Reproducibility and package management\n\nFor reproducibility, it's important to know the versions of the packages you use (and the version of Python).\n`pip` and `conda` make it easy to do this. You can create a *requirements* file that captures the packages you are currently using (and, critically, their versions) and then install exactly that set of packages (and versions) based on that requirements file.\n\n\n::: {.cell}\n\n```{.bash .cell-code}\npip freeze > requirements.txt\npip install -r requirements.txt\n\nconda env export > environment.yml\nconda env create -f environment.yml\n```\n:::\n\n\nConda is a general package manager. You can use it to manage Python packages but lots of other software as well, including R and Julia.\n\nConda environments provide an additional layer of modularity/reproducibility, allowing you to set up a fully reproducible environment for your computation. Here (by explicitly giving `python=3.12`) the Python 3.12 executable and all packages you install in the environment are fully independent of whatever Python executables are installed on the system.\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nconda create -n myenv python=3.12\nsource activate myenv\nconda install numpy\n```\n:::\n\n\n:::{.callout-warning}\nIf you use `conda activate` rather than `source activate`, Conda will prompt you to run `conda init`, which will make changes to your `~/.bashrc` that, for one, activate the Conda base environment automatically when a shell is started. This may be fine, but it's helpful to be aware.\n:::\n\n### Package locations\n\nPackages in Python (and in R, Julia, etc.) may be installed in various places\non the filesystem, and it sometimes it is helpful (e.g., if you end up with multiple\nversions of a package installed on your system) to be able to figure out \nwhere on the filesystem the package is being loaded from. \n\nWe can use the `__file__` and `__version__` objects in a package to see where on the filesystem a package is installed and what version it is:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport numpy as np\nnp.__file__\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'/system/linux/miniforge-3.13/lib/python3.13/site-packages/numpy/__init__.py'\n```\n\n\n:::\n\n```{.python .cell-code}\nnp.__version__\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'2.1.3'\n```\n\n\n:::\n:::\n\n\n(`pip list` or `conda list` will also show version numbers, for all packages.)\n\n`sys.path` shows where Python looks for packages on your system.\n\n### Source vs. binary packages\n\nThe difference between a *source* package and a *binary* package is that\nthe source package has the raw Python (and C/C++ and Fortran, in some cases) code\nas text files, while the binary package has all the non-Python code in a\nbinary/non-text format, with the  C/C++ and Fortran code already having been\ncompiled.\n\nIf you install a package from source, C/C++/Fortran code will be compiled on your system\n(if the package has such code).\nThat should mean the compiled code will work on your system, but requires you to have a\ncompiler available and things properly configured. A binary package doesn't need to be\ncompiled on your system, but in some cases the code may not run on your system because\nit was compiled in such a way that is not compatible with your system.\n\nPython *wheels* are a binary package format for Python packages. Wheels for some packages will vary by platform\n(i.e., operating system) so that the package will install correctly on the system where it is being installed.\n\n# 4. Types and data structures\n\n## Data structures\n\nPlease see the [data structures section of Unit 2](unit2-dataTech.html#data-structures) for some general discussion of data structures.\n\nWe'll also see more complicated data structures when we consider objects in the [section on object-oriented programming](#object-oriented-programming-oop).\n\n## Types and classes\n\n### Overview and static vs. dynamic typing\n\nThe term 'type' refers to how a given piece of information is stored and what operations can be done with the information.\n\n\n'Primitive' types are the most basic types that often relate directly to how data are stored in memory or on disk (e.g., boolean, integer, numeric (real-valued, aka *double* or *floating point*), character, pointer (aka *address*, *reference*).\n\nIn compiled languages like C and C++, one has to define the type of each variable. Such languages are *statically* typed.\nInterpreted (or scripting) languages such as Python and R have *dynamic* types. One can associate different types of information with a given variable name at different times and without declaring the type of the variable:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nx = 'hello'\nprint(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nhello\n```\n\n\n:::\n\n```{.python .cell-code}\nx = 7\nx*3\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n21\n```\n\n\n:::\n:::\n\n\nIn contrast in a language like C, one has to declare a variable based on its type before using it:\n\n\n::: {.cell}\n\n```{.c .cell-code}\ndouble y;\ndouble x = 3.1;\ny = x * 7.1;\n```\n:::\n\n\nDynamic typing can be quite helpful from the perspective of quick implementation and avoiding tedious type definitions and problems from minor inconsistencies between types (e.g., multiplying an integer by a real-valued number).\nBut static typing has some critical advantages from the perspective of software development, including:\n\n  - protecting against errors from mismatched values and unexpected user inputs, and\n  - generally much faster execution because the type of a variable does not need to be checked when the code is run.\n\nMore complex types in Python (and in R) often use references (*pointers*, aka *addresses*) to the actual locations of the data. We'll see this in detail when we discuss [Memory](#memory-and-copies).\n\n### Types in Python\n\nYou should be familiar with the important built-in data types in Python,\nmost importantly lists, tuples, and dictionaries, as well\nas basic scalar types such as integers, floats, and strings. \n\nLet's look at the type of various built-in data structures in Python and in numpy, which provides important types for numerical computing.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nx = 3\ntype(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<class 'int'>\n```\n\n\n:::\n\n```{.python .cell-code}\nx = 3.0\ntype(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<class 'float'>\n```\n\n\n:::\n\n```{.python .cell-code}\nx = 'abc'\ntype(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<class 'str'>\n```\n\n\n:::\n\n```{.python .cell-code}\nx = False\ntype(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<class 'bool'>\n```\n\n\n:::\n\n```{.python .cell-code}\nx = [3, 3.0, 'abc']\ntype(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<class 'list'>\n```\n\n\n:::\n\n```{.python .cell-code}\nimport numpy as np\n\nx = np.array([3, 5, 7])  ## array of integers\ntype(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<class 'numpy.ndarray'>\n```\n\n\n:::\n\n```{.python .cell-code}\ntype(x[0])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<class 'numpy.int64'>\n```\n\n\n:::\n\n```{.python .cell-code}\nx = np.random.normal(size = 3) # array of floats (aka 'doubles')\ntype(x[0])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<class 'numpy.float64'>\n```\n\n\n:::\n\n```{.python .cell-code}\nx = np.random.normal(size = (3,4)) # multi-dimensional array\ntype(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<class 'numpy.ndarray'>\n```\n\n\n:::\n:::\n\n\nSometimes numpy may modify a type to make things easier for you, which often works well, but you may want to control it yourself to be sure:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nx = np.array([3, 5, 7.3])\nx\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([3. , 5. , 7.3])\n```\n\n\n:::\n\n```{.python .cell-code}\ntype(x[0])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<class 'numpy.float64'>\n```\n\n\n:::\n\n```{.python .cell-code}\nx = np.array([3.0, 5.0, 7.0]) # Force use of floats (either `3.0` or `3.`).\ntype(x[0])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<class 'numpy.float64'>\n```\n\n\n:::\n\n```{.python .cell-code}\nx = np.array([3, 5, 7], dtype = 'float64')\ntype(x[0])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<class 'numpy.float64'>\n```\n\n\n:::\n:::\n\n\nThis can come up when working on a GPU, where the default is usually 32-bit (4-byte) numbers instead of 64-bit (8-byte) numbers. \n\n### Composite objects\n\nMany objects\ncan be *composite* (e.g., a list of dictionaries or a dictionary of lists,\ntuples, and strings).\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmydict = {'a': 3, 'b': 7}\nmylist = [3, 5, 7]\n\nmylist[1] = mydict\nmylist\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[3, {'a': 3, 'b': 7}, 7]\n```\n\n\n:::\n\n```{.python .cell-code}\nmydict['a'] = mylist\n```\n:::\n\n\n### Mutable objects\n\nMost objects in Python can be modified *in place* (i.e., modifying only some of the object), but tuples, strings, and sets are *immutable*:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nx = (3,5,7)\ntry:\n    x[1] = 4\nexcept Exception as error:\n    print(error)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'tuple' object does not support item assignment\n```\n\n\n:::\n\n```{.python .cell-code}\ns = 'abc'\ns[1]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'b'\n```\n\n\n:::\n\n```{.python .cell-code}\ntry:\n    s[1] = 'y'\nexcept Exception as error:\n    print(error)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'str' object does not support item assignment\n```\n\n\n:::\n:::\n\n\n### Converting between types\n\nThis also goes by the term *coercion* and *casting*. \nCasting often needs to be done explicitly in compiled languages and somewhat less so in interpreted languages like Python.\n\nWe can *cast* (coerce) between different basic types:\n\n\n::: {.cell}\n\n```{.python .cell-code}\ny = str(x[0])\ny\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'3'\n```\n\n\n:::\n\n```{.python .cell-code}\ny = int(x[0])\ntype(y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<class 'int'>\n```\n\n\n:::\n:::\n\n\nSome common conversions are converting numbers that are being\ninterpreted as strings into actual numbers and converting between booleans and numeric values.\n\nIn some cases Python will automatically do\nconversions behind the scenes in a smart way (or occasionally not so\nsmart way). Consider these attempts/examples of implicit coercion:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nx = np.array([False, True, True])\nx.sum()         # What do you think is going to happen?\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nnp.int64(2)\n```\n\n\n:::\n\n```{.python .cell-code}\nx = np.random.normal(size = 5)\ntry:\n    x[3] = 'hat'    # What do you think is going to happen?\nexcept Exception as error:\n    print(error)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ncould not convert string to float: 'hat'\n```\n\n\n:::\n\n```{.python .cell-code}\n  \nmyList = [1, 3, 5, 9, 4, 7]\n# myList[2.0]    # What do you think is going to happen?\n# myList[2.73]   # What do you think is going to happen?\n```\n:::\n\n\nR is less strict and will do conversions in some cases that Python won't:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- rnorm(5)\nx[2.0]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9974761\n```\n\n\n:::\n\n```{.r .cell-code}\nx[2.73]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9974761\n```\n\n\n:::\n:::\n\n\nQuestion: What are the advantages and disadvantages of the different behaviors of Python and R?\n\n### Dataframes\n\nHopefully you're also familiar with the Pandas dataframe type.\n\nPandas picked up the idea of dataframes from R and functionality is similar in many ways to\nwhat you can do with R's `dplyr` package.\n\n`dplyr` and `pandas` provide a lot of functionality for the \"split-apply-combine\"\nframework of working with \"rectangular\" data. Unfortunately, using Pandas can be a bit hard to learn/remember. I suggest looking into learning [`polars`](unit2-dataTech.html#reading-data-quickly-arrow-and-polars) as an alternative.\n\nOften analyses are done in a stratified fashion - the same operation or\nanalysis is done on subsets of the data set. The subsets might be\ndifferent time points, different locations, different hospitals,\ndifferent people, etc.\n\nThe split-apply-combine framework is intended to operate in this kind of\ncontext:\n  - first one splits the dataset by one or more variables,\n  - then one does something to each subset, and\n  - then one combines the results.\n\nsplit-apply-combine is also closely related to the famous Map-Reduce framework\nunderlying big data tools such as Hadoop and Spark. \n\nIt's also very similar to standard SQL queries involving filtering, grouping, and\naggregation.\n\n### Python object protocols\n\nThere are a number of broad categories of kinds of objects: `mapping`, `number`, `sequence`, `iterator`. These are called object protocols.\n\nAll objects that fall in a given category share key characteristics. For example `sequence` objects have a notion of \"next\", while\n`iterator` objects have a notion of \"stopping\".\n\nIf you implement your own class that falls into one of these categories, it should follow the relevant protocol\nby providing the required methods. For example a container class that supports iteration should provide the `__iter__` and `__next__` methods.\n\nHere we see that `tuple`s are iterable containers:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmytuple = (\"apple\", \"banana\", \"cherry\")\n\nfor item in mytuple:\n    print(item)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\napple\nbanana\ncherry\n```\n\n\n:::\n\n```{.python .cell-code}\n## We can manually create the iterator and iterate through it.\nmyit = iter(mytuple)\n## myit = mytuple.__iter__()  ## This is equivalent to using `iter(mytuple)`.\n\nprint(next(myit))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\napple\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(next(myit))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nbanana\n```\n\n\n:::\n\n```{.python .cell-code}\nmyit.__next__()   ## This is equivalent to using `next(myit)`.\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'cherry'\n```\n\n\n:::\n:::\n\n\nWe've actually gotten ahead of ourselves -- how is it that `iter` seems to do the same thing as `mytuple.__iter__()`\nand `next` seems to do the same thing as `myit.__next__()`? We'll discuss that in the next section when we discuss the [Python object model](#the-python-object-model-and-dunder-methods).\n\n\n::: {.cell}\n\n```{.python .cell-code}\nx = zip(['clinton', 'bush', 'obama', 'trump'], ['Dem', 'Rep', 'Dem', 'Rep'])\nnext(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n('clinton', 'Dem')\n```\n\n\n:::\n\n```{.python .cell-code}\nnext(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n('bush', 'Rep')\n```\n\n\n:::\n:::\n\n\nWe can also go from an iterable object to a standard list:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nr = range(5)\nr\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nrange(0, 5)\n```\n\n\n:::\n\n```{.python .cell-code}\nlist(r)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[0, 1, 2, 3, 4]\n```\n\n\n:::\n:::\n\n\n# 5. Programming paradigms: object-oriented and functional programming\n\nObject-oriented and functional programming are two important approaches to programming.\n\nFunctional programming (FP) focuses on writing functions that take inputs and produce outputs. Ideally those functions don't change the state (i.e., the values) of any variables and can be treated as black boxes. Functions can be treated like other variables, such as passing functions as arguments to another function (as one does with `map` in Python).\n\nObject-oriented programming (OOP) revolves around objects that belong to classes. The class of an object defines the fields (the data objects) holding information and  methods that can be applied to those fields. When one calls a method, it may modify the value of the fields. A statistical analogy is that an object of a class is like the realization (the object) of a random variable (the class).\n\nOne can think of functional programming as being focused on actions (or *verbs* to make an analogy with human language). One carries out a computation as a sequence of function calls. One can think of OOP as being focused on the objects (or *nouns*). One carries out a computation as a sequence of operations with the objects, using the class methods.\n\nMany languages are multi-paradigm, containing aspects of both approaches and allowing programmers to use either approach. Both R and Python are like this, though one would generally consider R to be more functional and Python to be more object-oriented.\n\nLet's illustrate the ideas with some numpy and list functionality.\n\n```python\nimport numpy as np\nx = np.array([1.2, 3.5, 4.2, 9.7])\nx.shape     # field (or attribute) of the numpy array class\nx.sum()     # method of the class\nnp.sum(x)   # equivalent numpy function\nlen(x)      # built-in function\n\n# functional approach: apply functions sequentially\nx2 = np.reshape(x, (2,2))\nx2t = np.transpose(x2)\n\n# functional, but using class methods\nx2 = x.reshape(2,2)\nx2t = x2.transpose()\n\n# OOP: modify objects using class methods\ny = list([1.2, 3.5, 4.2])\ny.append(7.9)  # y modified in place using class method\n```\n\nDifferent people have different preferences, but which is better sometimes depends on what you are trying to do. If your computation is a data analysis pipeline that involves a series of transformations of some data, a functional approach might make more sense, since the focus is on a series of actions rather than the state of objects. If your computation involves various operations on fixed objects whose state needs to change, OOP might make more sense. For example, if you were writing code to keep track of student information, it would probably make sense to have each student as an object of a `Student` class with methods such as `register` and `assign_grade`. \n\n# 6. Object-oriented programming (OOP)\n\nOOP involves organizing your code around objects that contain information, and methods that operate in specific ways on those objects.\nObjects belong to classes. A class is made up of fields (the data) that store information and methods (functions) that operate on the fields.\n\n\nBy analogy, OOP focuses on the nouns, with the verbs being part of the nouns, while FP focuses on the verbs (the functions), which operate on the nouns (the arguments).\n\n## Principles\n\nSome of the standard concepts in object-oriented programming include *encapsulation*, *inheritance*, *polymorphism*, and *abstraction*.\n\n*Encapsulation* involves preventing direct access to internal data in an object from outside the object. Instead the class is designed so that access (reading or writing) happens through the interface set up by the programmer (e.g., 'getter' and 'setter' methods). However, Python actually doesn't really enforce the notion of internal or private information.\n\n*Inheritance* allows one class to be based on another class, adding more specialized features.  For example in the `statsmodels` package, the OLS class inherits from the WLS class.\n\n*Polymorphism* allows for different behavior of an object or function depending on the context. A polymorphic function behaves differently depending on the input types. For example, think of a print function or an addition operator behaving differently depending on the type of the input argument(s). A polymorphic object is one that can belong to different classes (e.g., based on inheritance), and a given method name can be used with any of the classes. An example would be having a base or super class called 'algorithm' and various specific machine learning algorithms inheriting from that class. All of the classes might have a 'predict' method.\n\n*Abstraction* involves hiding the details of how something is done (e.g., via the method of a class), giving the user an interface to provide inputs and get outputs. By making the actual computation a black box, the programmer can modify the internals without changing how a user uses the system. \n\nClasses generally have *constructors* that initialize objects of the class and *destructors* that remove objects.\n\n## Classes in Python\n\nPython provides a pretty standard approach to writing object-oriented code focused on classes. \n\nOur example is to create a class for working with random time series. Each object of the class has specific parameter values that control the stochastic behavior of the time series. With a given object we can simulate one or more time series (realizations).\n\nHere's the initial definition of the class with methods and fields (aka attributes).\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport numpy as np\n\nclass tsSimClass:\n    '''\n    Class definition for time series simulator\n    '''\n    ## dunder methods (more later)\n    def __init__(self, times, mean = 0, cor_param = 1, seed = 1):\n        ## This is the constructor, called when `tsSimClass(...)` is invoked\n        ## to create an instance of the class.\n\n        ## For robustness, need checks that `cor_param` is numeric of length 1 and `times` is np array.\n        ## Public attributes\n        self.n = len(times)\n        self.mean = mean\n        self.cor_param = cor_param\n        ## Private attributes (encapsulation)\n        self._times = times\n        self._current_U = False\n        ## Some setup steps\n        self._calc_mats()\n        np.random.seed(seed)\n    def __str__(self):    # 'print' method\n        return f\"An object of class `tsSimClass` with {self.n} time points.\"\n    def __len__(self):\n        return self.n\n\n    ## Public methods: getter and setter (encapsulation)\n    def set_times(self, new_times):\n        self._times = new_times\n        self._current_U = False\n        self._calc_mats()\n    def get_times(self):\n        return self._times\n \n    ## Main public method\n    def simulate(self):\n        if not self._current_U:    \n            self._calc_mats()\n        ## analogous to mu+sigma*z for generating N(mu, sigma^2)\n        return self.mean + np.dot(self.U.T, np.random.normal(size = self.n))\n\n    ## Private method.\n    def _calc_mats(self):\n        ## Calculates correlation matrix and Cholesky factor (caching).\n        lag_mat = np.abs(self._times[:, np.newaxis] - self._times)\n        cor_mat = np.exp(-lag_mat ** 2 / self.cor_param ** 2)\n        self.U = np.linalg.cholesky(cor_mat)\n        print(\"Done updating correlation matrix and Cholesky factor.\")\n        self._current_U = True\n```\n:::\n\n\n\nNow let's see how we would use the class.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmyts = tsSimClass(np.arange(1, 101), 2, 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDone updating correlation matrix and Cholesky factor.\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(myts)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAn object of class `tsSimClass` with 100 time points.\n```\n\n\n:::\n\n```{.python .cell-code}\nnp.random.seed(1)\n## Here's a simulated time series.\ny1 = myts.simulate()\n\nimport matplotlib.pyplot as plt\nplt.plot(myts.get_times(), y1, '-')\nplt.xlabel('time')\nplt.ylabel('process values')\n## Simulate a second series.\ny2 = myts.simulate()\nplt.plot(myts.get_times(), y2, '--')\nplt.show()\n```\n\n::: {.cell-output-display}\n![](unit5-programming_files/figure-html/unnamed-chunk-61-1.png){fig-alt='Randomly-generated time series from our object' width=672}\n:::\n:::\n\n\nWe could set up a different object that has different parameter values.\nThat new simulated time series is less wiggly because the `cor_param` value\nis larger than before.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmyts2 = tsSimClass(np.arange(1, 101), 2, 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDone updating correlation matrix and Cholesky factor.\n```\n\n\n:::\n\n```{.python .cell-code}\nnp.random.seed(1)\n## Here's a simulated time series with a different value of\n## the correlation parameter (cor_param).\ny3 = myts2.simulate()\n\nplt.plot(myts2.get_times(), y3, '-', color = 'red')\nplt.xlabel('time')\nplt.ylabel('process values')\nplt.show()\n```\n\n::: {.cell-output-display}\n![](unit5-programming_files/figure-html/unnamed-chunk-62-3.png){fig-alt='Another randomly-generated time series' width=672}\n:::\n:::\n\n\n#### Copies and references\n\nNext let's think about when copies are made. In the next example `myts_ref` is a copy of `myts`\nin the sense that both names point to the same underlying object.\nBut no data were copied when the assignment to `myts_ref` was done. \n\n\n::: {.cell}\n\n```{.python .cell-code}\nmyts_ref = myts\n## 'myts_ref' and 'myts' are names for the same underlying object.\nimport copy\nmyts_full_copy = copy.deepcopy(myts)\n\n## Now let's change the values of a field.\nmyts.set_times(np.arange(1,1001,10))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDone updating correlation matrix and Cholesky factor.\n```\n\n\n:::\n\n```{.python .cell-code}\nmyts.get_times()[0:4] \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([ 1, 11, 21, 31])\n```\n\n\n:::\n\n```{.python .cell-code}\nmyts_ref.get_times()[0:4] # the same as `myts`\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([ 1, 11, 21, 31])\n```\n\n\n:::\n\n```{.python .cell-code}\nmyts_full_copy.get_times()[0:4] # different from `myts`\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([1, 2, 3, 4])\n```\n\n\n:::\n:::\n\n\nIn contrast `myts_full_copy` is a reference to a different object, and\nall the data from `myts` had to be copied over to `myts_full_copy`. This takes additional memory (and time), but is also safer, as it avoids the possibility that the user might modify `myts` and not realize that they were also affecting `myts_ref`. We'll discuss this more when we discuss copying in the [section on memory use](#memory-and-copies).\n\n#### Encapsulation\n\nThose of you familiar with OOP will probably be familiar with the idea of public and private fields and methods.\n\nWhy have private fields (i.e., *encapsulation*)? The use of private fields shields them from\n    modification by users. Python doesn't really provide this functionality but by convention,\n    attributes whose name starts with `_` are considered private.\n    In this case, we don't want users to modify the \n`times` field. Why is this important? In this example, the correlation matrix and\n    the Cholesky factor U are both functions of the array of times. So\n    we don't want to allow a user to directly modify `times`. If they did, it would leave the fields of the object in inconsistent states. Instead we want  them to use `set_times`, which correctly keeps all the fields\n    in the object internally consistent (by calling `_calc_mats`). It also allows us to improve efficiency\nby controlling when computationally expensive operations are carried out.\n\nIn a module, objects that start with `_` are a weak form of private attributes. Users can access them, but `from foo import *` does not import them.\n\n\n\n### Inheritance\n\nInheritance can be a powerful way to reduce code duplication and keep your code organized in a logical (nested) fashion.\nSpecial cases can be simple extensions of more general classes. A good example of inheritance in Python and R is how regression models are handled.\nE.g., in Python's `statsmodels` package, the [`OLS` class inherits from the `WLS` class](https://www.statsmodels.org/stable/_modules/statsmodels/regression/linear_model.html#OLS).\n\n\n::: {.cell}\n\n```{.python .cell-code}\nclass Bear:\n      def __init__(self, name, age):\n          self.name = name\n          self.age = age\n      def __str__(self):\n          return f\"A bear named '{self.name}' of age {self.age}.\"\n      def color(self):\n          return \"unknown\"\n\nclass GrizzlyBear(Bear):\n      def __init__(self, name, age, num_people_killed = 0):\n          super().__init__(name, age)\n          self.num_people_killed = num_people_killed\n      def color(self):  \n          return \"brown\"\n\nyog = Bear(\"Yogi the Bear\", 23)\nprint(yog)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nA bear named 'Yogi the Bear' of age 23.\n```\n\n\n:::\n\n```{.python .cell-code}\nyog.color()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'unknown'\n```\n\n\n:::\n\n```{.python .cell-code}\nnum399 = GrizzlyBear(\"Jackson Hole Grizzly 399\", 35)\nprint(num399)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nA bear named 'Jackson Hole Grizzly 399' of age 35.\n```\n\n\n:::\n\n```{.python .cell-code}\nnum399.color()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'brown'\n```\n\n\n:::\n\n```{.python .cell-code}\nnum399.num_people_killed\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0\n```\n\n\n:::\n:::\n\n\nHere the `GrizzlyBear` class has additional fields/methods beyond\nthose inherited from the base class (the `Bear` class),\ni.e., `num_people_killed` (since grizzly bears are much more dangerous\nthan some other kinds of bears), and perhaps additional or modified\nmethods. Python uses the methods specific to the `GrizzlyBear`\nclass if present before falling back to methods of the `Bear` class if not\npresent in the `GrizzlyBear` class.\n\nThe above is an example of polymorphism. Instances of the `GrizzlyBear` class are polymorphic\nbecause they can have behavior from both the `GrizzlyBear` and `Bear` classes. The `color` method is polymorphic\nin that it can be used for both classes but is defined to behave differently depending on the class.\n\n\n\n## Attributes\n\nBoth fields and methods are *attributes*.\n\nWe saw the notion of attributes when looking at HTML and XML, where the information was stored as\nkey-value pairs that in many cases had additional information in the form of attributes.\n\n### Class attributes vs. instance attributes\n\nHere `count` is a class attribute while `name` and `age` are instance attributes.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nclass Bear:\n      count = 0\n      def __init__(self, name, age):\n          self.name = name\n          self.age = age\n          Bear.count += 1\n\nyog = Bear(\"Yogi the Bear\", 23)\nyog.count\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n1\n```\n\n\n:::\n\n```{.python .cell-code}\nsmokey = Bear(\"Smokey the Bear\", 77)\nsmokey.count\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n2\n```\n\n\n:::\n:::\n\n\nThe class attribute allows us to manipulate information relating to all instances of the class, as seen here where we keep track of the number of bears that have been created.\n\n### Adding attributes\n\nWhat do you think will happen if we do the following?\n\n\n::: {.cell}\n\n```{.python .cell-code}\nyog.bizarre = 7\nyog.bizarre\n\ndef foo(x):\n    print(x)\n\nfoo.bizarre = 3\nfoo.bizarre\n```\n:::\n\n\nIt turns out we can add instance attributes on the fly in some cases, which is a bit disconcerting in some ways.\n\n## Generic function OOP\n\nLet's consider the `len` function in Python. It seems to work magically on various kinds of objects.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nx = [3, 5, 7]\nlen(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n3\n```\n\n\n:::\n\n```{.python .cell-code}\nx = np.random.normal(size = 5)\nlen(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n5\n```\n\n\n:::\n\n```{.python .cell-code}\nx = {'a': 2, 'b': 3}\nlen(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n2\n```\n\n\n:::\n:::\n\n\nSuppose you were writing the `len` function. What would you have to do to make it work as it did above?\nWhat would happen if a user wants to use `len` with a class that they define?\n\nInstead, Python implements the `len` function by calling the `__len__` method of the class that the argument belongs to.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nx = {'a': 2, 'b': 3}\nlen(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n2\n```\n\n\n:::\n\n```{.python .cell-code}\nx.__len__()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n2\n```\n\n\n:::\n:::\n\n\n`__len__` is a *dunder* method (a \"Double-UNDERscore\" method), which we'll discuss more in a bit.\n\nSomething similar occurs with operators:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nx = 3\nx + 5\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n8\n```\n\n\n:::\n\n```{.python .cell-code}\nx = 'abc'\nx + 'xyz'\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'abcxyz'\n```\n\n\n:::\n\n```{.python .cell-code}\nx.__add__('xyz')\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'abcxyz'\n```\n\n\n:::\n:::\n\n\nThis use of generic functions is convenient in that it allows us to work with a variety of kinds of objects using familiar functions.\n\nThe use of such generic functions and operators is similar in spirit to function or method *overloading* in C++ and Java.\nIt is also how the (very) old S3 system in R works. And it's a key part of the (fairly) new Julia language.\n\n### Why use generic functions?\n\nThe Python developers could have written `len` as a regular function with a bunch of `if` statements so that it can handle different kinds of input objects. \n\nThis has some disadvantages:\n\n  1. We need to write the code that does the checking.\n  2. Furthermore, all the code for the different cases all lives inside one potentially very long function, unless we create class-specific helper functions.\n  3. Most importantly, `len` will only work for existing classes. And users can't easily extend it for new classes that they create because they don't control the `len` (built-in) function. So a user could not add the additional conditions/classes in a big if-else statement. The generic function approach makes the system *extensible* -- we can build our own new functionality on top of what is already in Python. \n\n#### The print function\n\nLike `len`, `print` is a generic function, with various\nclass-specific methods.\n\nWe can write a print method for our own class by defining the `__str__`\nmethod as well as a `__repr__` method giving what to display when the name of an object is typed.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nclass Bear:\n      def __init__(self, name, age):\n          self.name = name\n          self.age = age\n\nyog = Bear(\"Yogi the Bear\", 23)\nprint(yog)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<__main__.Bear object at 0x7a42079578c0>\n```\n\n\n:::\n\n```{.python .cell-code}\nclass Bear:\n      def __init__(self, name, age):\n          self.name = name\n          self.age = age\n      def __str__(self):\n          return f\"A bear named {self.name} of age {self.age}.\"\n      def __repr__(self):\n          return f\"Bear(name={self.name}, age={self.age})\"\n\nyog = Bear(\"Yogi the Bear\", 23)\nprint(yog)   # Invokes __str__\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nA bear named Yogi the Bear of age 23.\n```\n\n\n:::\n\n```{.python .cell-code}\nyog          # Invokes __repr__\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBear(name=Yogi the Bear, age=23)\n```\n\n\n:::\n:::\n\n\n\n### Multiple dispatch OOP \n\nThe dispatch system involved in `len` and `+` involves only the first argument to the function (or operator). In contrast, [Julia emphasizes the importance of multiple dispatch](https://docs.julialang.org/en/v1/manual/methods) as particularly important for mathematical computation. With multiple dispatch, the specific method can be chosen based on more than one argument.\n\nIn R, the old (but still used in some contexts) [S4](http://adv-r.had.co.nz/S4.html) system in R and the new [R7](https://rconsortium.github.io/OOP-WG) system both provide for multiple dispatch. \n\nAs a very simple example unrelated to any specific language, multiple dispatch would allow one to do the following with the addition operator:\n\n```\n3 + 7    # 10\n3 + 'a'  # '3a'\n'hi' +  ' there'  # 'hi there'\n```\n\nThe idea of having the behavior of an operator or function adapt to the type of the input(s) is one aspect of *polymorphism*.\n\n## The Python object model and *dunder* methods\n\nNow that we've seen the basics of classes, as well as generic function OOP, we're in a good position to understand the Python object model.\n\nObjects are dictionaries that provide a mapping from attribute names to their values, either fields or methods. \n\n*dunder* methods are special methods that Python will invoke when various functions are called on instances of the class or other standard operations are invoked. They allow classes to interact with Python's built-ins.\n\nHere are some important dunder methods:\n\n - `__init__` is the constructor (initialization) function that is called when the class name is invoked (e.g., `Bear(...)`)\n - `__len__` is called by `len()`\n - `__str__` is called by `print()`\n - `__repr__` is called when an object's name is invoked\n - `__call__` is called if the instance is invoked as a function call (e.g., `yog()` in the `Bear` case)\n - `__add__` is called by the `+` operator.\n - `__getitem__` is called by the `[` slicing operator.\n\nLet's see an example of defining a dunder method for the `Bear` class.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nclass Bear:\n      def __init__(self, name, age):\n          self.name = name\n          self.age = age\n      def __str__(self):\n          return f\"A bear named {self.name} of age {self.age}.\"\n      def __add__(self, value):\n          self.age += value\n\nyog = Bear(\"Yogi the Bear\", 23)\nyog + 12\nprint(yog)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nA bear named Yogi the Bear of age 35.\n```\n\n\n:::\n:::\n\n\n\nMost of the things we work with in Python are objects.\nFunctions are also objects, as are classes.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntype(len)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<class 'builtin_function_or_method'>\n```\n\n\n:::\n\n```{.python .cell-code}\ndef foo(x):\n    print(x)\n\ntype(foo)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<class 'function'>\n```\n\n\n:::\n\n```{.python .cell-code}\ntype(Bear)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<class 'type'>\n```\n\n\n:::\n:::\n\n\n:::{.callout-tip title=\"Challenge\"}\nLet's check our understanding of the object model.\n\nHow would you get Python to quit immediately, without asking for any more information, when you simply type `q` (no parentheses!) instead of `quit()`? (Hint: you can do this by understanding what happens when you type `q` and how to exploit the characteristics of Python classes.)\n:::\n\n# 7. Functional programming\n\nThis section covers an approach to programming called *functional programming* as well as various\nconcepts related to writing and using functions. \n\n## Functional programming (in Python)\n\n### Overview of functional programming\n\nFunctional programming is an approach to programming that emphasizes the use of modular, self-contained functions.\nSuch functions should operate only on arguments provided to\nthem (avoiding global variables), and **produce no side effects**, although in some cases there are good\nreasons for making an exception. Another aspect of functional programming is \nthat functions are considered 'first-class' citizens in that they can be passed as arguments to another function,\nreturned as the result of a function, and assigned to variables. In other words, a function can be treated as any other variable.\n\nIn many cases (including Python and R), anonymous functions (also called 'lambda functions') can be created on-the-fly for use in various circumstances. \n\nOne can do functional programming in Python by focusing on writing modular, self-contained functions rather than classes. And functions are first-class citizens. However, there are aspects of Python that do not align with the principles mentioned above.\n\n  - Python's pass-by-reference behavior causes functions to potentially have the important side effects of modifying arguments that are mutable (e.g., lists and numpy arrays but not tuples) if the programmer is not careful about not modifying arguments within functions.\n  - Some operations are carried out by statements (e.g., `import`, `def`) rather than functions.\n\nIn contrast, R functions have pass-by-value behavior, which is more consistent with a pure functional programming approach.\n\n\n### The principle of no side effects\n\nBefore we discuss Python further, let's consider how R behaves in more detail as R conforms more strictly to a functional programming perspective.\n\nMost functions available in R (and ideally functions that you write as well) operate by taking in arguments and producing output that is then (presumably) used subsequently. The functions generally don't have any effect on the state of your R environment/session other than the output they produce.\n\nAn important reason for this (plus for not using global variables) is that it means that it is easy for people using the language to understand what code does. Every function can be treated a black box -- you don't need to understand what happens in the function or worry that the function might do something unexpected (such as changing the value of one of your variables). The result of running code is simply the result of a composition of functions, as in mathematical function composition.\n\nOne aspect of this is that R uses a *pass-by-value* approach to function arguments.\nIn R (but not Python), when you pass an object in as an argument and then modify it in the function, you are modifying a local copy of the variable that exists in the context (the *frame*) of the function and is deleted when the function call finishes:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- 1:3\nmyfun <- function(x) {\n      x[2] <- 7\n      print(x)\n      return(x)\n}\n\nnew_x <- myfun(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1 7 3\n```\n\n\n:::\n\n```{.r .cell-code}\nx   # unmodified\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1 2 3\n```\n\n\n:::\n:::\n\n\nIn contrast, Python uses a *pass-by-reference* approach, seen here:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nx = np.array([1,2,3])\ndef myfun(x):\n  x[1] = 7\n  return x\n\nnew_x = myfun(x)\nx   # modified!\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([1, 7, 3])\n```\n\n\n:::\n:::\n\n\nAnd actually, given the pass-by-reference behavior, we would probably use a version of `myfun`\nthat looks like this:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nx = np.array([1,2,3])\ndef myfun(x):\n  x[1] = 7\n  return None\n\nmyfun(x)\nx   # modified!\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([1, 7, 3])\n```\n\n\n:::\n:::\n\n\n\nNote how easy it would be for a Python programmer to violate the 'no side effects' principle.\nIn fact to avoid it, we need to do some additional work in terms of making a copy of `x`\nto a new location in memory before modifying it in the function.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nx = np.array([1,2,3])\ndef myfun(x):\n  y = x.copy()\n  y[1] = 7\n  return y\n\nnew_x = myfun(x)\nx   # no side effects!\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([1, 2, 3])\n```\n\n\n:::\n:::\n\n\nMore on [pass-by-value vs. pass-by-reference](#pass-by-value-vs.-pass-by-reference) later.\n\nEven in R, there are some (necessary) exceptions to the idea of no side effects, such as `par()`, `library()`,  and `plot()`.\n\n### Functions are first-class objects\n\nEverything in Python is an object, including functions and classes. We can assign\nfunctions to variables in the same way we assign numeric and other\nvalues.\n\nWhen we make an assignment we associate a name (a 'reference') with an object in memory.\nPython can find the object by using the name to look up the object in the namespace.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nx = 3\ntype(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<class 'int'>\n```\n\n\n:::\n\n```{.python .cell-code}\ntry:\n    x([1,3,5])  # x is not a function (yet)\nexcept Exception as error:\n    print(error)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'int' object is not callable\n```\n\n\n:::\n\n```{.python .cell-code}\nx = sum\n\nx([1,3,5])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n9\n```\n\n\n:::\n\n```{.python .cell-code}\ntype(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<class 'builtin_function_or_method'>\n```\n\n\n:::\n:::\n\n\nWe can call a function based on the text name of the function.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfunction = getattr(np, \"mean\")\nfunction(np.array([1,2,3]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nnp.float64(2.0)\n```\n\n\n:::\n:::\n\n\nWe can also pass a function into another function as the actual function\nobject. This is an important aspect of functional programming.\nWe can do it with our own function or (as we'll see shortly) with various built-in functions, such as `map`.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef apply_fun(fun, a):\n    return fun(a)\n\napply_fun(round, 3.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n4\n```\n\n\n:::\n:::\n\n\nA function that takes a function as an argument, returns a function as a result, or both is known as a *higher-order function*.\n\n### Which operations are function calls?\n\nPython provides various statements that are not formal function calls but allow one to modify the current Python session:\n\n - `import`: import modules or packages\n - `def`: define functions or classes\n - `return`: return results from a function\n - `del`: remove an object\n\nAs we saw earlier with `+` (`__add__`), operators are examples of generic function OOP, where the appropriate method of the class of the first operand is called.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nx = np.array([0,1,2])\nx - 1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([-1,  0,  1])\n```\n\n\n:::\n\n```{.python .cell-code}\nx.__sub__(1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([-1,  0,  1])\n```\n\n\n:::\n\n```{.python .cell-code}\nx\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([0, 1, 2])\n```\n\n\n:::\n:::\n\n\nNote that the use of the operator does not modify the object.\n\n(Note that you can use `return(x)` and `del(x)` but behind the scenes the Python interpreter is intepreting those\nas `return x` and `del x`.)\n\n### Map operations\n\nA *map* operation takes a function and runs the function on each element of some collection of items,\nanalogous to a mathematical map. This kind of operation is very commonly used in programming, particularly functional programming,\nand often makes for clean, concise, and readable code.\n\nPython provides a variety of map-type functions: `map` (a built-in) and `pandas.apply`. These are examples of higher-order functions -- functions that take a function as an argument. Another map-type operation is *list comprehension*, shown here:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nx = [1,2,3]\ny = [pow(val, 2) for val in x]\ny\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1, 4, 9]\n```\n\n\n:::\n:::\n\n\nIn Python, `map` is run on the elements of an *iterable* object. Such objects include lists  as well as the result of `range()` and other functions that produce iterables.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nx = [1.0, -2.7, 3.5, -5.1]\nlist(map(abs, x))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1.0, 2.7, 3.5, 5.1]\n```\n\n\n:::\n\n```{.python .cell-code}\nlist(map(pow, x, [2,2,2,2]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1.0, 7.290000000000001, 12.25, 26.009999999999998]\n```\n\n\n:::\n:::\n\n\nOr we can use *lambda* functions to define a function on the fly:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nx = [1.0, -2.7, 3.5, -5.1]\nresult = list(map(lambda vals: vals * 2, x))\n```\n:::\n\n\nA lambda function is a temporary function that is defined \"on-the-fly\" rather than in advance and is never given a name. (These are also sometimes called *anonymous* functions.)\n\nIf you need to pass another argument to the function you can use a lambda function as above or `functools.partial`:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom functools import partial\n\n# Create a new round function with 'ndigits' argument pre-set\nround3 = partial(round, ndigits = 3)\n\n# Apply the function to a list of numbers\nlist(map(round3, [32.134234, 7.1, 343.7775]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[32.134, 7.1, 343.777]\n```\n\n\n:::\n:::\n\n\n\nLet's compare using a map-style operation (with Pandas) to using a for loop to run a stratified analysis for a generic example (this code won't run because the variables don't exist):\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# stratification \nsubsets = df.groupby('grouping_variable')\n\n# map using pandas.apply: one line, easy to understand\nresults = subsets.apply(analysis_function)\n\n# for loop: needs storage set up and multiple lines\nresults <- []\nfor _,subset in subsets:   # iterate over the key-value pairs (the subsets)\n  results.append(analysis_function(subset))\n```\n:::\n\n\nMap operations are also at the heart of the famous *MapReduce* framework, used in Hadoop and Spark for big data processing. \n\n## Function evaluation, frames, and the call stack\n\n### Overview\n\nWhen we run code, we end up calling functions inside of other function calls.\nThis leads to a nested series of function calls. The series of calls is the *call stack*.\nThe stack operates like a stack of cafeteria trays - when a\nfunction is called, it is added to the stack (pushed) and when it\nfinishes, it is removed (popped).\n\nUnderstanding the series of calls is important when reading error messages and debugging.\nIn Python, when an error occurs, the call stack is shown, which has the advantage of\ngiving the complete history of what led to the error and the disadvantage of producing\noften very verbose output that can be hard to understand. (In contrast, in R, only the function in which\nthe error occurs is shown, but you can see the full call stack by invoking `traceback()`.)\n\nWhat happens when an Python function is evaluated?\n\n  - The user-provided function arguments are evaluated in the calling scope and the results are\nmatched to the argument names in the function definition. \n  - A new frame containing a new namespace is created to store information related to the function call and placed on the stack. Assignment to the argument names is done in the namespace, including any default arguments. \n  - The function is evaluated in the (new) local scope. Any look-up of variables not found in the\nlocal scope (using the namespace that was created) is done using the lexical scoping rules to look in the series of enclosing scopes (if any exist), then in the global/module scope, and then in the built-ins scope. \n  - When the function finishes, the return value is passed back to the calling scope and the frame is\ntaken off the stack. The namespace is removed, unless the namespace is the enclosing scope for an existing namespace.\n\nI'm not expecting you to fully understand that previous paragraph and\nall the terms in it yet. We'll see all the details as we proceed through this Unit.\n\n### Frames and the call stack\n\nPython keeps track of the call stack. Each function call is associated with\na frame that has a *namespace* that contains the local variables for that function call.\n\nThere are a bunch of functions that let us query what frames are on the\nstack and access objects in particular frames of interest. This gives us\nthe ability to work with objects in the frame from which a function was\ncalled.\n\nWe can use functions from the `traceback` package to query the call stack.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport traceback\n\ndef function_a():\n    function_b()  # line 2 within function, line 4 overall\n\ndef function_b():\n    # some comment\n    # another comment\n    function_c()  # line 4 within function, line 9 overall\n\ndef function_c():\n    traceback.print_stack() # line 2 within, line 12 overall\n    # raise RuntimeError(\"A fake error\")\n\nfunction_a()    # line 1 relative to the call, line 15 overall\n```\n:::\n\n\nIf we run that in Python (not via rendering the qmd file directly, because the line numbering shown is strange when things go through the quarto rendering process), we see this:\n\n```\n  File \"<stdin>\", line 1, in <module>\n  File \"<stdin>\", line 2, in function_a\n  File \"<stdin>\", line 4, in function_b\n  File \"<stdin>\", line 2, in function_c\n```\n\nIf we run the code by putting it in a module, say `file.py` and running `python file.py`, we see this, with the line numbering relative to the overall file, but showing the same stack of function calls:\n\n```\n  File \"file.py\", line 15, in <module>\n    function_a()    # line 1 relative to the call, line 15 overall\n  File \"file.py\", line 4, in function_a\n    function_b()  # line 2 within function, line 4 overall\n  File \"file.py\", line 9, in function_b\n    function_c()  # line 4 within function, line 9 overall\n  File \"file.py\", line 12, in function_c\n    traceback.print_stack() # line 2 within, line 12 overall\n```\n\nIf we comment out the `print_stack()` call and uncomment the error, we see the same traceback information. That's exactly what is happening when you get a long series of information when Python stops with an error.\n\n\n## Function inputs and outputs \n\n### Arguments\n\nYou can see the arguments (and any default values) for a function using the help system.\n\nLet's create an example function:\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef add(x, y, z=1, absol=False):\n    if absol:\n        return abs(x+y+z)\n    else:\n        return x+y+z\n```\n:::\n\n\nWhen defining a function, arguments without defaults must come first.\n\nWhen using a function, there are some rules that must be followed.\n\nFirst, users must provide values for arguments without defaults.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nadd(3, 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n9\n```\n\n\n:::\n\n```{.python .cell-code}\nadd(3, 5, 7)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n15\n```\n\n\n:::\n\n```{.python .cell-code}\nadd(3, 5, absol=True, z=-5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n3\n```\n\n\n:::\n\n```{.python .cell-code}\nadd(z=-5, x=3, y=5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n3\n```\n\n\n:::\n\n```{.python .cell-code}\ntry:\n    add(3)\nexcept Exception as error:\n    print(error)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nadd() missing 1 required positional argument: 'y'\n```\n\n\n:::\n:::\n\n\nSecond, arguments can be specified by position (based on the order of the inputs)\nor by name (keyword), using `name=value`. The user needs to provide\npositional arguments first.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nadd(z=-5, 3, 5)  ## Can't trap `SyntaxError` with `try`\n# SyntaxError: positional argument follows keyword argument  \n```\n:::\n\n\n\nFunctions may have unspecified arguments, which are designated using `*args`.\n('args' is a convention - you can call it something else).\nUnspecified arguments occurring at the beginning of the argument\nlist are generally a collection of like objects that will be manipulated\n(consider `print`).\n\nHere's an example where we see that we can manipulate *args*, which is a tuple,\nas desired.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef sum_args(*args):\n    print(args[2])\n    total = sum(args)\n    return total\n\nresult = sum_args(1, 2, 3, 4, 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n3\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(result)  # Output: 15\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n15\n```\n\n\n:::\n:::\n\n\nThis syntax also comes in handy for some existing functions, such as\n`os.path.join`, which can take either an arbitrary number of inputs or a list.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nos.path.join('a','b','c')\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'a/b/c'\n```\n\n\n:::\n\n```{.python .cell-code}\nx = ['a','b','c']\nos.path.join(*x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'a/b/c'\n```\n\n\n:::\n:::\n\n\n`*args` only handles positional arguments. You'd need to also have `**kwargs` as an argument to handle keyword arguments. In the function, `args` will be a tuple while `kwargs` will be a dictionary.\n\n### Function outputs\n\n`return x` will specify `x` as the output of the function.  `return` can occur anywhere in the function, and\nallows the function to exit as soon as it is done.\n\nWe can return multiple outputs using `return` - the return value will then be a tuple.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef f(x):\n    if x < 0:\n        return -x**2\n    else:\n        res = x^2\n        return x, res\n\n\nf(-3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n-9\n```\n\n\n:::\n\n```{.python .cell-code}\nf(3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(3, 1)\n```\n\n\n:::\n\n```{.python .cell-code}\nout1,out2 = f(3)\n```\n:::\n\n\n\nIf you want a function to be invoked for its side effects, you can omit `return` or\nexplicitly have `return None` or simply `return`.\n\n\n## Pass by value vs. pass by reference\n\nWhen talking about programming languages, one often distinguishes\n*pass-by-value* and *pass-by-reference*. \n\n*Pass-by-value* means that when a\nfunction is called with one or more arguments, a copy is made of each\nargument and the function operates on those copies. In pass-by-value, changes to an\nargument made within a function do not affect the value of the argument\nin the calling environment. \n\n*Pass-by-reference*\nmeans that the arguments are not copied, but rather that information is\npassed allowing the function to find and modify the original value of\nthe objects passed into the function. In pass-by-reference changes inside a\nfunction can affect the object outside of the function. \n\nPass-by-value is elegant and modular in that functions do not have side\neffects - the effect of the function occurs only through the return\nvalue of the function. However, it can be inefficient in terms of the\namount of computation and of memory used. In contrast, pass-by-reference\nis more efficient, but also more dangerous and less modular. It's more\ndifficult to reason about code that uses pass-by-reference because\neffects of calling a function can be hidden inside the function.\nThus pass-by-value is directly related to functional programming.\n\nArrays and other non-scalar objects in Python are pass-by-reference (but note that tuples are immutable, \nso one could not modify a tuple that is passed as an argument).\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef myfun(x):\n    x[1] = 99\n\ny = [0, 1, 2]\nz = myfun(y)\ntype(z)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<class 'NoneType'>\n```\n\n\n:::\n\n```{.python .cell-code}\ny\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[0, 99, 2]\n```\n\n\n:::\n:::\n\n\n\n\nLet's see what operations cause arguments modified in a function to affect state outside of the function:\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef myfun(f_scalar, f_x, f_x_new, f_x_newid, f_x_copy):\n\n    # `scalar` in global unaffected, as `f_scalar` is redefined.\n    f_scalar = 99              \n\n    # `x` in global is MODIFIED, as `x` and `f_x` have same id.\n    f_x[0] = 99                 \n\n    # `x_new` in global unaffected as `f_x_new` is redefined.\n    f_x_new = [99,2,3]        \n\n    # `x_newid` in global is MODIFIED as `x_newid` and `y` have same id.\n    y = f_x_newid\n    y[0] = 99              \n\n    # `x_copy` in global is unaffected as `x_copy` has different id from `z`\n    z = f_x_copy.copy() \n    z[0] = 99              \n\n\nscalar = 1\nx = [1,2,3]\nx_new = [1,2,3]\nx_newid = [1,2,3]\nx_copy = [1,2,3]\n\n\nmyfun(scalar, x, x_new, x_newid, x_copy)\n```\n:::\n\n\nHere are the cases where state is preserved:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nscalar\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n1\n```\n\n\n:::\n\n```{.python .cell-code}\nx_new\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1, 2, 3]\n```\n\n\n:::\n\n```{.python .cell-code}\nx_copy\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1, 2, 3]\n```\n\n\n:::\n:::\n\n\nAnd here are the cases where state is modified:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nx\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[99, 2, 3]\n```\n\n\n:::\n\n```{.python .cell-code}\nx_newid\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[99, 2, 3]\n```\n\n\n:::\n:::\n\n\nBasically if you replace the reference (object name) then the state outside the function is preserved.\nThat's because a new local variable in the function scope is created. If you modify part of the object, state is not preserved.\n\nThe same behavior occurs with other mutable objects such as numpy arrays.\n\n### Pointers (optional)\n\nTo put pass-by-value vs. pass-by-reference in a broader context, I want to briefly discuss\nthe idea of a pointer, common in compiled languages such as C.\n\n\n::: {.cell}\n\n```{.c .cell-code}\nint x = 3;\nint* ptr;\nptr = &x;\n*ptr * 7; // returns 21\n```\n:::\n\n\n  - `int x=3` declares `x` to be an int and immediately defines it to have the value 3.\n  - The `int*` declares `ptr` to be a pointer to (the address of) the integer `x`.\n  - `&x` gets the address where `x` is stored.\n  - `*ptr` dereferences `ptr`, returning the value in that address (which is 3 since `ptr` is the address of `x`.\n\nArrays in C are really pointers to a block of memory:\n\n\n::: {.cell}\n\n```{.c .cell-code}\nint x[10];\n```\n:::\n\n\nIn this case `x` will be the address of the first element of the array.\nWe can access the first element as `x[0]` or `*x`.\n\nWhy have we gone into this? In C, you can pass a pointer as an argument\nto a function. The result is that only the scalar address is copied and\nnot the entire object, and inside the function, one can modify the\noriginal object, with the new value persisting on exit from the\nfunction. For example in the following example one passes in the address of an\nobject and that object is then modified in place, affecting its value when the function\ncall finishes.\n\n\n::: {.cell}\n\n```{.c .cell-code}\nint myCal(int* ptr){\n    *ptr = *ptr + *ptr;\n}\n\nmyCal(&x)  # x itself will be modified\n```\n:::\n\n\nSo Python behaves similarly to the use of pointers in C.\n\n\n## Namespaces and scopes\n\nAs discussed [here in the Python docs](https://docs.python.org/3/tutorial/classes.html#python-scopes-and-namespaces),\na *namespace* is a mapping from names to objects that allows Python to find objects by name via clear rules that\nenforce modularity and avoid name conflicts.\n\nNamespaces are created and removed through the course of executing Python code. When a function is run, a namespace for the local\nvariables in the function is created, and then deleted when the function finishes executing. Separate function calls\n(including recursive calls) have separate namespaces. \n\n*Scope* is closely related concept -- a scope determines what namespaces are accessible from a given place in one's code. Scopes are nested and determine where and in what order Python searches the various namespaces for objects. \n\nNote that the ideas of namespaces and scopes are relevant in most other languages, though the details of how they work can differ.\n\nThese ideas are very important for modularity, isolating the names of objects to avoid conflicts.\n\nThis allows you to use the same name in different modules or submodules, as well as different packages using the same name.\n\nOf course to make the objects in a module or package available we need to use `import`.\n\nConsider what happens if you have two modules that both use `x` and you import `x` using `from`.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom mypkg.mymod import x\nfrom mypkg.mysubpkg import x\nx  # which x is used?\n```\n:::\n\n\nWe've added `x` twice to the namespace of the global scope. Are both available? Did one 'overwrite' the other? How do I access the other one?\n\nThis is much better:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport mypkg\nmypkg.x\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n7\n```\n\n\n:::\n\n```{.python .cell-code}\nimport mypkg.mysubpkg\nmypkg.mysubpkg.x\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n999\n```\n\n\n:::\n:::\n\n\nSide note: notice that `import mypkg` causes the name `mypkg` itself to be in the current (global) scope.\n\n\nWe can see the objects in a given namespace/scope using `dir()`.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nxyz = 7\ndir()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n['Bear', 'GrizzlyBear', '__annotations__', '__builtins__', '__doc__', '__loader__', '__name__', '__package__', '__spec__', 'add', 'apply_fun', 'content', 'copy', 'f', 'files', 'foo', 'function', 'io', 'it', 'item', 'm', 'math', 'myList', 'mydict', 'myfun', 'myit', 'mylist', 'mymod', 'mypkg', 'myts', 'myts2', 'myts_full_copy', 'myts_ref', 'mytuple', 'new_x', 'np', 'num399', 'os', 'out1', 'out2', 'partial', 'pattern', 'platform', 'plt', 'r', 're', 'result', 'return_group', 'round3', 's', 'scalar', 'smokey', 'st', 'stream', 'subprocess', 'sum_args', 'sys', 'text', 'time', 'tmp', 'tsSimClass', 'x', 'x_copy', 'x_new', 'x_newid', 'xyz', 'y', 'y1', 'y2', 'y3', 'yog', 'z']\n```\n\n\n:::\n\n```{.python .cell-code}\nimport mypkg\ndir(mypkg)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'auxil', 'myfun', 'myfun10', 'mymod', 'mysubpkg', 'x']\n```\n\n\n:::\n\n```{.python .cell-code}\nimport mypkg.mymod\ndir(mypkg.mymod)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_helper', 'myfun', 'myfun10', 'x']\n```\n\n\n:::\n\n```{.python .cell-code}\nimport builtins\ndir(builtins)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n['ArithmeticError', 'AssertionError', 'AttributeError', 'BaseException', 'BaseExceptionGroup', 'BlockingIOError', 'BrokenPipeError', 'BufferError', 'BytesWarning', 'ChildProcessError', 'ConnectionAbortedError', 'ConnectionError', 'ConnectionRefusedError', 'ConnectionResetError', 'DeprecationWarning', 'EOFError', 'Ellipsis', 'EncodingWarning', 'EnvironmentError', 'Exception', 'ExceptionGroup', 'False', 'FileExistsError', 'FileNotFoundError', 'FloatingPointError', 'FutureWarning', 'GeneratorExit', 'IOError', 'ImportError', 'ImportWarning', 'IndentationError', 'IndexError', 'InterruptedError', 'IsADirectoryError', 'KeyError', 'KeyboardInterrupt', 'LookupError', 'MemoryError', 'ModuleNotFoundError', 'NameError', 'None', 'NotADirectoryError', 'NotImplemented', 'NotImplementedError', 'OSError', 'OverflowError', 'PendingDeprecationWarning', 'PermissionError', 'ProcessLookupError', 'PythonFinalizationError', 'RecursionError', 'ReferenceError', 'ResourceWarning', 'RuntimeError', 'RuntimeWarning', 'StopAsyncIteration', 'StopIteration', 'SyntaxError', 'SyntaxWarning', 'SystemError', 'SystemExit', 'TabError', 'TimeoutError', 'True', 'TypeError', 'UnboundLocalError', 'UnicodeDecodeError', 'UnicodeEncodeError', 'UnicodeError', 'UnicodeTranslateError', 'UnicodeWarning', 'UserWarning', 'ValueError', 'Warning', 'ZeroDivisionError', '_', '_IncompleteInputError', '__build_class__', '__debug__', '__doc__', '__import__', '__loader__', '__name__', '__package__', '__spec__', 'abs', 'aiter', 'all', 'anext', 'any', 'ascii', 'bin', 'bool', 'breakpoint', 'bytearray', 'bytes', 'callable', 'chr', 'classmethod', 'compile', 'complex', 'copyright', 'credits', 'delattr', 'dict', 'dir', 'divmod', 'enumerate', 'eval', 'exec', 'exit', 'filter', 'float', 'format', 'frozenset', 'getattr', 'globals', 'hasattr', 'hash', 'help', 'hex', 'id', 'input', 'int', 'isinstance', 'issubclass', 'iter', 'len', 'license', 'list', 'locals', 'map', 'max', 'memoryview', 'min', 'next', 'object', 'oct', 'open', 'ord', 'pow', 'print', 'property', 'quit', 'range', 'repr', 'reversed', 'round', 'set', 'setattr', 'slice', 'sorted', 'staticmethod', 'str', 'sum', 'super', 'tuple', 'type', 'vars', 'zip']\n```\n\n\n:::\n:::\n\n\nHere are the key scopes to be aware of, in order (\"LEGB\") of how the namespaces are searched:\n\n - **L**ocal scope: objects available within function (or class method).\n - non-local (**E**nclosing) scope: objects available from functions enclosing a given function (we'll talk about this more later; this relates to *lexical scoping*).\n - **G**lobal (aka 'module') scope: objects available in the module in which the function is defined (which may simply be the default global scope when you start the Python interpreter). This is also the local scope if the code is not executing inside a function.\n - **B**uilt-ins scope: objects provided by Python through the built-ins module but available from anywhere.\n\nNote that `import` adds the name of the imported module to the namespace of the current (i.e., local) scope.\n\nWe can see the local and global namespaces using `locals()` and `globals()`.\n\n\n::: {.cell}\n\n```{.bash .cell-code}\ncat local.py\n```\n\n\n::: {.cell-output .cell-output-stdout}\n\n```\ngx = 7\n\ndef myfun(z):\n    y = z*3\n    print(\"local: \", locals())\n    print(\"global: \", globals())\n```\n\n\n:::\n:::\n\n\nRun the following code to see what is in the different namespaces:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport local\n\ngx = 99\nlocal.myfun(3)\n```\n:::\n\n\nStrangely (for me being more used to R, where package namespaces are 'locked' such that objects can't be added), we can add an object to a namespace created from a module or package:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmymod.x = 33\ndir(mymod)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'myfun', 'range', 'x']\n```\n\n\n:::\n\n```{.python .cell-code}\nimport numpy as np\nnp.x = 33\n'x' in dir(np)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrue\n```\n\n\n:::\n:::\n\n\nAs more motivation, consider this example. \n\nSuppose we have this code in a module named `test_scope.py`:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\ncat test_scope.py\n```\n\n\n::: {.cell-output .cell-output-stdout}\n\n```\nmagic_number = 2\n\ndef transform(x):\n    return x*5\n\ndef myfun(val):\n    return(transform(val) * magic_number)\n```\n\n\n:::\n:::\n\n\nNow suppose we also define `magic_number` and `transform()` in the scope in which `myfun` is called from.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport test_scope\nmagic_number = 900\ndef transform(x):\n   print(\"haha\")\n\ntest_scope.myfun(3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n30\n```\n\n\n:::\n:::\n\n\nWe see that Python uses `magic_number` and `transform()` from the module.\nWhat would be bad about using `magic_number` or `transform()` from the scope of the Python session\nrather than the scope of the module? \n\nConsider a case where instead of using the `test_scope.py` module we were using\ncode from a package and that code has a variable called `magic_number` or  function called `transform`.\n\n### Lexical scoping (enclosing scopes)\n\nIn this section, we seek to understand what happens in the following\ncircumstance. Namely, where does Python get the value for the object `x`?\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef f(y):\n  return x + y\n\nf(3)\n```\n:::\n\n\n\nVariables in the enclosing scope are available within a function. \n **The enclosing scope is the\nscope in which a function is defined, not the scope from\nwhich a function is called.**\n\nThis approach is called *lexical scoping*. R and many other languages\nalso use lexical scoping.\n\nThe behavior of looking up object names based on where functions are defined rather than where they are called from extends the local-global scoping discussed in the previous section, with similar motivation.\n\n\nLet's dig deeper to understand where Python looks for non-local variables, illustrating lexical scoping:\n\n\n::: {.cell}\n\n```{.python .cell-code}\n## Case 1\nx = 3\ndef f2():\n    print(x)\n\ndef f():\n    x = 7\n    f2()\n\nf() # what will happen?\n\n## Case 2\nx = 3\ndef f2()\n    print(x)\n\ndef f():\n    x = 7\n    f2()\n\nx = 100\nf() # what will happen?\n\n## Case 3\nx = 3\ndef f():\n    def f2():\n        print(x)        \n    x = 7\n    f2()\n\nx = 100\nf() # what will happen?\n\n## Case 4\nx = 3\ndef f():\n    def f2():\n        print(x)\n    f2()\n\nx = 100\nf() # what will happen?\n```\n:::\n\n\nHere's a tricky example:\n\n\n::: {.cell}\n\n```{.python .cell-code}\ny = 100\ndef fun_constructor():\n\ty = 10\n\tdef g(x):\n            return x + y     \n\treturn g\n\n## fun_constructor() creates functions\nmyfun = fun_constructor()\nmyfun(3)\n```\n:::\n\n\nLet's work through this:\n\n1.  What is the enclosing scope for the function `g()`?\n2.  Which `y` does `g()` use?\n3.  Where is `myfun` defined (this is tricky -- how does `myfun` relate to `g`)?\n4.  What is the enclosing scope for `myfun()`?\n5.  When `fun_constructor()` finishes, does its namespace disappear?\n    What would happen if it did?\n6.  What does `myfun` use for `y`?\n\n\nWe can use the `inspect` package to see information about the closure.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport inspect\ninspect.getclosurevars(myfun)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nClosureVars(nonlocals={}, globals={}, builtins={}, unbound={'copy'})\n```\n\n\n:::\n:::\n\n\n(Note that I haven't fully investigated the use of `inspect`,\nbut it looks like it has a lot of useful tools.)\n\nBe careful when using variables from non-local scopes as the\nvalue of that variable may well not be what\nyou expect it to be. In general one wants to think carefully before using variables that\nare taken from outside the local scope, but in some cases it can be useful.\n\nNext we'll see some ways of accessing variables outside of the local scope.\n\n### Global and non-local variables\n\nWe can create and modify global variables and variables in the enclosing scope using\n`global` and `nonlocal` respectively. Note that *global* is in the context of the current module\nso this could be a variable in your current Python session if you're working with functions defined\nin that session or a global variable in a module or package.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndel x\n\ndef myfun():\n    global x\n    x = 7\n\nmyfun()\nprint(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n7\n```\n\n\n:::\n\n```{.python .cell-code}\nx = 9\nmyfun()\nprint(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n7\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef outer_function():\n    x = 10  # Outer variable\n    def inner_function():\n        nonlocal x\n        x = 20  # Modify the outer variable.\n    print(x)  # Output: 10\n    inner_function()\n    print(x)  # Output: 20\n\nouter_function()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n10\n20\n```\n\n\n:::\n:::\n\n\nIn R, one can do similar things using the global assignment operator `<<-`.\n\n### Closures\n\nOne way to associate data with functions is to use a *closure*.\nThis is a functional programming way to\nachieve something like an OOP class. This [Wikipedia\nentry](https://en.wikipedia.org/wiki/Closure_(computer_programming))\nnicely summarizes the idea, which is a general functional programming idea and not specific to Python. \n\nUsing a closure\ninvolves creating one (or more functions) within a function call and\nreturning the function(s) as the output. When one executes the original\nfunction (the constructor), the new function(s) is created and returned and one can then\ncall that function(s). The function then can access objects in\nthe enclosing scope (the scope of the constructor) and\ncan use `nonlocal` to assign into the enclosing scope, to which the\nfunction (or the multiple functions) have access. The nice thing about\nthis compared to using a global variable is that the data in the closure\nis bound up with the function(s) and is protected from being changed by\nthe user. \n\n\n::: {.cell}\n\n```{.python .cell-code}\nx = np.random.normal(size = 5)\ndef scaler_constructor(input):\n\tdata = input\n\tdef g(param):\n            return param * data\n\treturn g\n\nscaler = scaler_constructor(x)\ndel x # to demonstrate we no longer need x\nscaler(3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([ 0.5081473 ,  2.22166935, -2.86110181, -0.79865552,  0.09784364])\n```\n\n\n:::\n\n```{.python .cell-code}\nscaler(6)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([ 1.0162946 ,  4.44333871, -5.72220361, -1.59731104,  0.19568728])\n```\n\n\n:::\n:::\n\n\nSo calling `scaler(3)` multiplies 3 by the value of `data` stored in the closure (the namespace of the enclosing scope) of the function `scaler`.\n\nNote that it can be hard to see the memory use involved in the closure.\n\nHere's a more realistic example. There are other ways you could do this, but this is slick:\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef make_container(n):\n    x = np.zeros(n)\n    i = 0\n    def store(value = None):\n        nonlocal x, i\n        if value is None:\n            return x\n        else:\n            x[i] = value\n            i += 1\n    return store\n\n\nnboot = 20\nbootmeans = make_container(nboot)\n\nimport pandas as pd\niris = pd.read_csv('https://raw.githubusercontent.com/pandas-dev/pandas/master/pandas/tests/io/data/csv/iris.csv')\ndata = iris['SepalLength']\n\nfor i in range(nboot): \n    bootmeans(np.mean(np.random.choice(data, size = len(data), replace = True)))\n\n\nbootmeans()\n\nbootmeans.__closure__\n```\n:::\n\n\n## Decorators\n\nNow that we've seen function generators, it's straightforward to discuss *decorators*.\n\nA decorator is a wrapper around a function that extends the functionality of the function without actually modifying the function.\n\nWe can create a simple decorator \"manually\" like this:\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef verbosity_wrapper(myfun):\n    def wrapper(*args, **kwargs):\n        print(f\"Starting {myfun.__name__}.\")\n        output = myfun(*args, **kwargs)\n        print(f\"Finishing {myfun.__name__}.\")\n        return output\n    return wrapper\n    \nverbose_rnorm = verbosity_wrapper(np.random.normal)\n\nx = verbose_rnorm(size = 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStarting normal.\nFinishing normal.\n```\n\n\n:::\n\n```{.python .cell-code}\nx\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([-1.37311732,  0.31515939,  0.84616065, -0.85951594,  0.35054598])\n```\n\n\n:::\n:::\n\n\nPython provides syntax that helps you create decorators with less work (this is an example of the general idea of *syntactic sugar*).\n\nWe can easily apply our decorator defined above to a function as follows. Now the function name refers to the wrapped version of the function.\n\n\n::: {.cell}\n\n```{.python .cell-code}\n@verbosity_wrapper\ndef myfun(x):\n    return x\n\ny = myfun(7)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStarting myfun.\nFinishing myfun.\n```\n\n\n:::\n\n```{.python .cell-code}\ny\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n7\n```\n\n\n:::\n:::\n\n\nOur decorator doesn't do anything useful, but hopefully you can imagine that the idea of being able to have more control over the operation of functions could be useful. For example we could set up a timing wrapper so that when we run a function, we get a report on how long it took to run the function. Or using the idea of a closure, we could keep a running count of the number of times a function has been called.\n\nOne real-world example of using decorators is in setting up functions to run in parallel in `dask`, which we'll discuss in Unit 7. Another is the use of Numba to do [just-in-time (JIT) compilation](#just-in-time-jit-compilation) of Python code.\n\n# 8. Memory and copies\n\n\n## Overview\n\nThe main things to remember when thinking about memory use are: (1)\nnumeric arrays take 8 bytes per element and (2) we need to keep track\nof when large objects are created, including local variables in the\nframes of functions.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nx = np.random.normal(size = 5)\nx.itemsize # 8 bytes\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n8\n```\n\n\n:::\n\n```{.python .cell-code}\nx.nbytes\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n40\n```\n\n\n:::\n:::\n\n\n### Allocating and freeing memory\n\nUnlike compiled languages like C, in Python we do not need to explicitly\nallocate storage for objects. (However, we will see that there are times\nthat we do want to allocate storage in advance, rather than successively\nconcatenating onto a larger object.)\n\nPython automatically manages memory, releasing memory back to the operating\nsystem when it's not needed via a process called *garbage collection*. Very occasionally\nyou may want to remove large objects as soon as they are not needed.\n`del` does not actually free up memory, it just disassociates the name\nfrom the memory used to store the object. In general Python will quickly\nclean up such objects without a reference (i.e., a name), so there is generally\nno need to call `gc.collect()` to force\nthe garbage collection. \n\nIn a language like C in which the user allocates and frees up memory,\nmemory leaks are a major cause of bugs. Basically if you are looping and\nyou allocate memory at each iteration and forget to free it, the memory\nuse builds up inexorably and eventually the machine runs out of memory.\nIn Python, with automatic garbage collection, this is generally not an issue,\nbut occasionally memory leaks could occur.\n\n### The heap and the stack\n\nThe *heap* is the memory that is available for dynamically creating new\nobjects while a program is executing, e.g., if you create a new object\nin Python or call *new* in C++. When more memory is needed the program can\nrequest more from the operating system. When objects are removed in Python, Python\nwill handle the garbage collection of releasing that memory.\n\nThe *stack* is the memory used for local variables when a function is\ncalled. I.e., the stack is the memory associated with function frames.\nHence the term \"stack trace\" to refer to the active function frames\nduring execution of code.\n\nThere's a nice discussion of this on [this Stack Overflow\nthread](https://stackoverflow.com/questions/79923/what-and-where-are-the-stack-and-heap).\n\n\n## Monitoring memory use\n\n### Monitoring overall memory use on a UNIX-style computer\n\nTo understand how much memory is available on your computer, one needs\nto have a clear understanding of disk caching. The operating system will\ngenerally cache files/data in memory when it reads from disk. Then if\nthat information is still in memory the next time it is needed, it will\nbe much faster to access it the second time around than if it had to\nread the information from disk. While the cached information is using\nmemory, that same memory is immediately available to other processes, so\nthe memory is available even though it is \"in use\".\n\nWe can see this via `free -h` (the `-h` is for 'human-readable', i.e.\nshow in GB (G)) on Linux machine.\n\n```\n          total used free shared buff/cache available \n    Mem:   251G 998M 221G   2.6G        29G      247G \n    Swap:  7.6G 210M 7.4G\n```\n\nYou'll generally be interested in the `Mem` row. (See below for some\ncomments on `Swap`.) The `shared` column is complicated and probably\nwon't be of use to you. The `buff/cache` column shows how much space is\nused for disk caching and related purposes but is actually available.\nHence the `available` column is the sum of the `free` and `buff/cache`\ncolumns (more or less). In this case only about 1 GB is in use\n(indicated in the `used` column).\n\n`top` (Linux or Mac) and `vmstat` (on Linux) both show overall memory\nuse, but remember that the amount actually available to you is the\namount free plus any buff/cache usage. Here is some example output\nfrom `vmstat`:\n\n```\n\n    procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- \n    r b   swpd      free   buff    cache si so bi bo in cs us sy id wa st \n    1 0 215140 231655120 677944 30660296  0  0  1  2  0  0 18  0 82  0  0\n```\n\nIt shows 232 GB free and 31 GB used for cache and therefore available,\nfor a total of 263 GB available.\n\nHere are some example lines from `top`:\n\n```\n    KiB Mem : 26413715+total, 23180236+free, 999704 used, 31335072 buff/cache \n    KiB Swap:  7999484 total,  7784336 free, 215148 used. 25953483+avail Mem\n```\n\nWe see that this machine has 264 GB RAM (the total column in the `Mem`\nrow), with 259.5 GB available (232 GB free plus 31 GB buff/cache as seen\nin the `Mem` row). (I realize the numbers don't quite add up for reasons\nI don't fully understand, but we probably don't need to worry about that\ndegree of exactness.) Only 1 GB is in use.\n\n*Swap* is essentially the reverse of disk caching. It is disk space that\nis used for memory when the machine runs out of physical memory. You\nnever want your machine to be using swap for memory because your jobs\nwill slow to a crawl. As seen above, the `swap` line in both `free` and\n`top` shows 8 GB swap space, with very little in use, as desired.\n\n### Monitoring memory use in Python\n\nThere are a number of ways to see how much memory is being used. When Python\nis actively executing statements, you can use `top` from the UNIX shell.\n\nIn Python, we can call out to the system to get the info we want:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport psutil\n\n# Get memory information\nmemory_info = psutil.Process().memory_info()\n\n# Print the memory usage\nprint(\"Memory usage:\", memory_info.rss/10**6, \" Mb.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMemory usage: 313.319424  Mb.\n```\n\n\n:::\n\n```{.python .cell-code}\n\n# Let's turn that into a function for later use:\ndef mem_used():\n    print(\"Memory usage:\", psutil.Process().memory_info().rss/10**6, \" Mb.\")\n```\n:::\n\n\nWe can see the size of an object (in bytes) with `sys.getsizeof()`.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmy_list = [1, 2, 3, 4, 5]\nsys.getsizeof(my_list)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n104\n```\n\n\n:::\n\n```{.python .cell-code}\nx = np.random.normal(size = 10**7) # should use about 80 Mb\nsys.getsizeof(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n80000112\n```\n\n\n:::\n:::\n\n\nHowever, we need to be careful about objects that refer to other objects:\n\n\n::: {.cell}\n\n```{.python .cell-code}\ny = [3, x]\nsys.getsizeof(y)  # Whoops!\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n72\n```\n\n\n:::\n:::\n\n\n\nHere's a\ntrick where we serialize the object, as if to export it, and then see\nhow long the binary representation is.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pickle\nser_object = pickle.dumps(y)\nsys.getsizeof(ser_object)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n80000202\n```\n\n\n:::\n:::\n\n\nThere are also some flags that one can start `python` with that allow\none to see information about memory use and allocation. See `man python`.\nYou could also look into the `memory_profiler` or `pympler` packages. \n\n## How memory is used in Python\n\n### Two key tools: `id` and `is`\n\nWe can use the `id` function to see where in\nmemory an object is stored and `is` to see if two\nobject are actually the same objects in memory.\nIt's particularly useful for understanding storage\nand memory use for complicated data structures.\nWe'll also see that they can be handy tools for\nseeing where copies are made and where they are not.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nx = np.random.normal(size = 10**7)\nid(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n134424012222704\n```\n\n\n:::\n\n```{.python .cell-code}\nsys.getsizeof(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n80000112\n```\n\n\n:::\n\n```{.python .cell-code}\ny = x\nid(y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n134424012222704\n```\n\n\n:::\n\n```{.python .cell-code}\nx is y\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrue\n```\n\n\n:::\n\n```{.python .cell-code}\nsys.getsizeof(y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n80000112\n```\n\n\n:::\n\n```{.python .cell-code}\nz = x.copy()\nid(z)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n134424189314096\n```\n\n\n:::\n\n```{.python .cell-code}\nsys.getsizeof(z)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n80000112\n```\n\n\n:::\n:::\n\n\n\n### Memory use in specific circumstances\n\n#### How lists are stored\n\nHere we can use `id` to determine how the overall list is stored as\nwell as the elements of the list.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nnums = np.random.normal(size = 5)\nobj = [nums, nums, np.random.normal(size = 5), ['adfs']]\n\nid(nums)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n134424012222224\n```\n\n\n:::\n\n```{.python .cell-code}\nid(obj)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n134424011624256\n```\n\n\n:::\n\n```{.python .cell-code}\nid(obj[0])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n134424012222224\n```\n\n\n:::\n\n```{.python .cell-code}\nid(obj[1])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n134424012222224\n```\n\n\n:::\n\n```{.python .cell-code}\nid(obj[2])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n134424012222032\n```\n\n\n:::\n\n```{.python .cell-code}\nid(obj[3])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n134424012385088\n```\n\n\n:::\n\n```{.python .cell-code}\nid(obj[3][0])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n134424013724944\n```\n\n\n:::\n\n```{.python .cell-code}\nobj[0] is obj[1]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrue\n```\n\n\n:::\n\n```{.python .cell-code}\nobj[0] is obj[2]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFalse\n```\n\n\n:::\n:::\n\n\nWhat do we notice?\n\n  - The list itself appears to be a array of references (pointers) to the component elements.\n  - Each element has its own address.\n  - Two elements of a list can use the same memory (see the first two elements here, whose contents are at the same memory address).\n  - A list element can use the same memory as another object (or part of another object).\n  \n\nA note about calling `id` on list elements, e.g., `id(obj[0])`. Running that code causes execution of `obj[0]`, and the result is passed into `id`. That creates a new temporary object that is passed to `id`. The temporary object references the same object that `obj[0]` does, so we find out the id of `obj[0]`. \n\n#### How character strings are stored.\n\nSimilar tricks are used for storing strings (and also integers). We may explore this in a problem set problem.\n\n#### How numpy arrays are stored\n\nTo do vectorized calculations and linear algebra efficiently, one needs the data in arrays stored contiguously in memory, and this is how numpy arrays are stored. We've seen that a numpy array involves the actual numbers plus 112 bytes of metadata/overhead associated with the object. \n\nWe can't usefully call `id` on elements of the array. Consider this:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nx = np.random.normal(size=5)\ntype(x[1])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<class 'numpy.float64'>\n```\n\n\n:::\n\n```{.python .cell-code}\nid(x[1])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n134424012266864\n```\n\n\n:::\n\n```{.python .cell-code}\ntmp1 = x[1]\ntmp2 = x[1]\nid(tmp1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n134424012263152\n```\n\n\n:::\n\n```{.python .cell-code}\nid(tmp2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n134424012261328\n```\n\n\n:::\n:::\n\n\nEach time we get the `x[1]` element we are creating a new numpy float64 scalar object. This is because each element of the array is not its own object (unlike a list element) so extracting the element involves copying the value and making a new object. \n\nAnother indication of the array being stored contiguously is that if we pickle the array, its size is just a bit more than the size needed just to store the 8-byte numbers. If we instead pickle a list containing those same numbers, the result is more than twice as big, related to the pointers involved in referring to the individual numbers. \n\n#### Modifying elements in place\n\nWhat do this simple experiment tell us?\n\n\n::: {.cell}\n\n```{.python .cell-code}\nx = np.random.normal(size = 5)\nid(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n134424012222416\n```\n\n\n:::\n\n```{.python .cell-code}\nx[2] = 3.5\nid(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n134424012222416\n```\n\n\n:::\n:::\n\n\nIt makes some sense that modifying elements of an object here doesn't cause a copy -- if it did, working with large objects would be very difficult. \n\n### When are copies made?\n\nLet's try to understand when Python uses additional memory for objects, and how it knows when it can delete memory.\nWe'll use large objects so that we can use `free` or `top` to see how memory use by the Python process changes.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nx = np.random.normal(size = 10**8)\nid(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n134424012221648\n```\n\n\n:::\n\n```{.python .cell-code}\ny = x\nid(y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n134424012221648\n```\n\n\n:::\n\n```{.python .cell-code}\nx = np.random.normal(size = 10**8)\nid(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n134424012221168\n```\n\n\n:::\n:::\n\n\nOnly if we re-assign `x` to reference a different object does additional memory get used.\n\n\n#### How does Python know when it can free up memory?\n\nPython keeps track of how many names refer to an object and only\nremoves memory when there are no remaining references to an object.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport sys\n\nx = np.random.normal(size = 10**8)\ny = x\nsys.getrefcount(y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n3\n```\n\n\n:::\n\n```{.python .cell-code}\ndel x\nsys.getrefcount(y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n2\n```\n\n\n:::\n\n```{.python .cell-code}\ndel y\n```\n:::\n\n\nWe can see the number of references using `sys.getrefcount`.\nConfusingly, the number is one higher than we'd expect,\nbecause it includes the temporary reference from passing\nthe object as the argument to `getrefcount`.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nx = np.random.normal(size = 5)\nsys.getrefcount(x)  # In reality, only 1. \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n2\n```\n\n\n:::\n\n```{.python .cell-code}\ny = x\nsys.getrefcount(x)  # In reality, 2.\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n3\n```\n\n\n:::\n\n```{.python .cell-code}\nsys.getrefcount(y)  # In reality, 2.\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n3\n```\n\n\n:::\n\n```{.python .cell-code}\ndel y\nsys.getrefcount(x)  # In reality, only 1. \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n2\n```\n\n\n:::\n\n```{.python .cell-code}\ny = x\nx = np.random.normal(size = 5)\nsys.getrefcount(y)  # In reality, only 1. \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n2\n```\n\n\n:::\n\n```{.python .cell-code}\nsys.getrefcount(x)  # In reality, only 1. \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n2\n```\n\n\n:::\n:::\n\n\n\n\nThis notion of reference counting occurs in other contexts, such as\nshared pointers in C++ and in how R handles copying and garbage collection.\n\n## Strategies for saving memory\n\nA few basic strategies for saving memory include:\n\n-   Avoiding unnecessary copies.\n-   Removing objects that are not being used, at which point the Python garbage collector should free up the memory.\n-   Use iterators (e.g., `range()`) and [generators](https://wiki.python.org/moin/Generators) to avoid building long lists that are iterated over.\n\n\nIf you're really trying to optimize memory use, you may also consider:\n\n-   Using types that take up less memory (e.g., `Bool`, `Int16`, `Float32`) when\n    possible.\n\n    ::: {.cell}\n    \n    ```{.python .cell-code}\n    x = np.array(np.random.normal(size = 5), dtype = \"float32\")\n    x.itemsize\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    4\n    ```\n    \n    \n    :::\n    \n    ```{.python .cell-code}\n    x = np.array([3,4,2,-2], dtype = \"int16\")\n    x.itemsize\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    2\n    ```\n    \n    \n    :::\n    :::\n\n-   Reading data in from files in chunks rather than reading the entire dataset (more in Unit 7).\n-   Exploring packages such as `arrow` for efficiently using memory, as discussed in [Unit 2](unit2-dataTech.html#reading-data-quickly-arrow-and-polars).\n    \n\n## Example\n\nLet's work through a real example where we keep a running tally of\ncurrent memory in use and maximum memory used in a function call. We'll\nwant to consider hidden uses of memory and when copies of the object are made rather than simply creating a new reference to an existing object.  This\ncode (translated from the original R code) comes from a PhD student's research. For our purposes here, let's assume\nthat the input `x` and `y` are very long numpy arrays using a lot of memory. \n\n`np.isnan` returns an boolean array of True/False values indicating whether each input element is a NaN (not a number).\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef fastcount(xvar, yvar):\n    naline = np.isnan(xvar)\n    naline[np.isnan(yvar)] = True\n    localx = xvar.copy()\n    localy = yvar.copy()\n    localx[naline] = 0\n    localy[naline] = 0\n    useline = ~naline\n    ## We'll ignore the rest of the code.\n    ## ....\n\nfastcount(x, y) # Assume x, y are existing large numpy arrays.\n```\n:::\n\n\n:::{.callout-tip title=\"Challenge\"}\nIn the code above, note all the places where new memory must be allocated.\n:::\n\n:::{.callout-tip title=\"Solution\" collapse=\"true\"}\n\n - Line 1: `xvar` and `yvar` are local variables that are references to `x` and `y`.\n - Line 2: `naline` is a new object\n - Line 3: `np.isnan(yvar)` creates a temporary boolean array that should be garbage-collected (its memory freed) very quickly once that line of code is finished executing. It increases memory use only temporarily. `naline` is modified in place.\n - Lines 4-5: `localx` and `localy` are new objects.\n - Lines 6-7: `localx` and `localy` are modified in place without a new object being created.\n - Line 8: `useline` is a new object.\n:::\n\n# 9. Efficiency\n\n## Interpreters and compilation\n\n### Why are interpreted languages slow?\n\nCompiled code runs quickly because the original code has been translated into instructions (machine language) that the processor can understand (i.e., zeros and ones). In the process of doing so, various checking and lookup steps are done once and don't need to be redone when running the compiled code.\n\nIn contrast, when one runs code in an interpreted language such as Python or R, the interpreter needs to do all the checking and lookup each time the code is run. This is required because the types and locations in memory of the variables could have changed.\n\nWe'll focus on Python in the following discussion, but most of the concepts apply to other interpreted languages.\n\nFor example, consider this code:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nx = 3\nabs(x)\nx*7\n\nx = 'hi'\nabs(x)\nx*3\n```\n:::\n\n\nBecause of dynamic typing, when the interpreter sees `abs(x)` it needs to check if `x` is something to which the absolute value function can be applied, including dealing with the fact that `x` could be a list or array with many numbers in it. In addition it needs to (using scoping rules) look up the value of `x`. (Consider that `x` might not even exist at the point that `abs(x)` is called.) Only then can the absolute value calculation happen. For the multiplication, Python needs to lookup the version of `*` that can be used, depending on the type of `x`.\n\nLet's consider writing a loop with some ridiculous code:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nx = np.random.normal(10)\nfor i in range(10):\n    if np.random.normal(size = 1) > 0:\n        x = 'hi'\n    if np.random.normal(size = 1) > 0.5:\n        del x\n    x[i]= np.exp(x[i])\n```\n:::\n\n\nThere is no way around the fact that because of how dynamic this is, the interpreter needs to check if `x` exists, if it is an array of sufficient length, if it contains numeric values, and it needs to go retrieve the required value, EVERY TIME the `np.exp()` is executed. Now the code above is unusual, and in most cases, we wouldn't have the `if` statements that modify `x`. So you could imagine a process by which the checking were done on the first iteration and then not needed after that -- that gets into the idea of just-in-time compilation, discussed later. \n\nThe standard Python interpreter (CPython) is a C function so in some sense everything that happens is running as compiled code, but there are lots more things being done to accomplish a given task using interpreted code than if the task had been written directly in code that is compiled. By analogy, consider talking directly to a person in a language you both know compared to talking to a person via an interpreter who has to translate between two languages. Ultimately, the same information gets communicated (hopefully!) but the number of words spoken and time involved is much greater. \n\nWhen running more complicated functions, there is often a lot of checking that is part of the function itself. For example scipy's `solve_triangular` function ultimately calls out to the `trtrs` Lapack function, but before doing so, there is a lot of checking that can take time. To that point, the documentation suggests you might set `check_finite=False` to improve performance at the expense of potential problems if the input matrices contain troublesome elements.\n\nWe can flip the question on its head and ask what operations in an interpreted language will execute quickly. In Python, these include:\n\n- operations that call out to compiled C code,\n- linear algebra operations (these call out to compiled C or Fortran code provided by the BLAS and LAPACK software packages), and\n- vectorized calls rather than loops:\n   - vectorized calls generally run loops in compiled C code rather than having the loop run in Python, and\n   - that means that the interpreter doesn't have to do all the checking discussed above for every iteration of the loop.\n\n\n\n### Compilation\n\n#### Overview\n\nCompilation is the process of turning code in a given language (such a C++) into machine code. Machine code is the code that the processor actually executes. The machine code is stored in the executable file, which is a binary file. The history of programming has seen ever great levels of abstraction, so that humans can write code using syntax that is easier for us to understand, re-use, and develop building blocks that can be put together to do complicated tasks. For example assembly language is a step above machine code. Languages like C and Fortran provide additional abstraction beyond that. The Statistics 750 class at CMU has a [nice overview](https://36-750.github.io/tools/computer-architecture/#how-programs--aka-apps--run) if you want to see more details.\n\nNote that interpreters such as Python are themselves programs -- the standard Python interpreter (CPython) is a C program that has been compiled. It happens to be a program that processes Python code. The interpreter doesn't turn Python code into machine code, but the interpreter itself is machine code.\n\n#### Just-in-time (JIT) compilation\n\nStandard compilation (ahead-of-time or AOT compilation) happens before any code is executed and can involve a lot of optimization to produce the most efficient machine code possible.\n\nIn contrast, just-in-time (JIT) compilation happens at the time that the code is executing. JIT compilation is heavily used in Julia, which is very fast (in some cases as fast as C). JIT compilation involves translating to machine code as the code is running. One nice aspect is that the results are cached so that if code is rerun, the compilation process doesn't have to be redone. So if you use a language like Julia, you'll see that the speed can vary drastically between the first time and later times you run a given function during a given session.\n\nOne thing that needs to be dealt with is type checking. As discussed above, part of why an interpreter is slow is because the type of the variable(s) involved in execution of a piece of code is not known in advance, so the interpreter needs to check the type. In JIT systems, there are often type inference systems that determine variable types. \n\nJIT compilation can involve translation from the original code to machine code or translation of bytecode (see next section) to machine code.\n\nAt the end of this unit, we'll see the use of JIT compilation with the JAX package in Python.\n\n`numba` is a standard JIT compiler for Python and numpy that uses the LLVM compiler library. To use it one applies the `numba.njit` decorator to a Python function.\n\n\n#### Byte compiling (optional)\n\nFunctions in Python and Python packages may byte compiled. What does that mean?\nByte-compiled code is a special representation that can be\nexecuted more efficiently because it is in the form of compact codes\nthat encode the results of parsing and semantic analysis of scoping and\nother complexities of the Python source code. This byte code can be executed\nfaster than the original Python code because it skips the stage of having to\nbe interpreted by the Python interpreter.\n\nIf you look at the file names in the directory of an installed Python\npackage you may see files with the `.pyc` extension.\nThese files have been byte-compiled.\n\nWe can byte compile our own functions using either the `py_compile` or `compileall`\nmodules. Here's an\nexample (silly since as experienced Python programmers, we would use\nvectorized calculation here rather than this unvectorized code.)\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport time\n\ndef f(vals):\n    x = np.zeros(len(vals))\n    for i in range(len(vals)):\n        x[i] = np.exp(vals[i])\n    return x\n\nx = np.random.normal(size = 10**6)\nt0 = time.time()\nout = f(x)\ntime.time() - t0\n```\n:::\n\n```\n0.7561802864074707\n```\n\n\n::: {.cell}\n\n```{.python .cell-code}\nt0 = time.time()\nout = np.exp(x)\ntime.time() - t0\n```\n:::\n\n```\n0.013027191162109375\n```\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport py_compile\npy_compile.compile('vec.py')\n```\n:::\n\n\n```\n'__pycache__/vec.cpython-312.pyc'\n```\n\n\n::: {.cell}\n\n```{.bash .cell-code}\ncp __pycache__/vec.cpython-312.pyc vec.pyc\nrm vec.py    # Make sure non-compiled module not loaded.\n```\n:::\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport vec\nvec.__file__\n```\n:::\n\n```\n'/accounts/vis/paciorek/teaching/243fall24/fall-2024/units/vec.pyc'\n```\n\n\n::: {.cell}\n\n```{.python .cell-code}\nt0 = time.time()\nout = vec.f(x)\ntime.time() - t0\n```\n:::\n\n```\n0.7301280498504639\n```\n\nUnfortunately, as seen above byte compiling may not speed things up much. I'm not sure why.\n\n\n## Benchmarking and profiling\n\nRecall that it's a waste of time to optimize code before you determine  (1) that the code is too slow for how it will be used and (2) which are the slow steps on which to focus your attempts to speed the code up. A 100x speedup in a step that takes 1% of the time will speed up the overall code by essentially nothing.\n\n### Timing your code\n\nThere are a few ways to time code:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport time\nx = np.random.normal(size = 10**7)\nt0 = time.time()\ny = np.exp(x)\nt1 = time.time()\n\nprint(f\"Execution time: {t1-t0} seconds.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nExecution time: 0.07062745094299316 seconds.\n```\n\n\n:::\n:::\n\n\nIn general, it's a good idea to repeat (replicate) your timing, as there is some stochasticity in how fast your computer will run a piece of code at any given moment.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport time\nt0 = time.time()\ny = np.exp(3.)\nt1 = time.time()\n\nprint(f\"Execution time: {t1-t0} seconds.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nExecution time: 0.005770683288574219 seconds.\n```\n\n\n:::\n:::\n\n\nUsing `time` is fine for code that takes a little while to run, but for code that is really fast (such as the code above), it may not be very accurate. Measuring fast bits of code is tricky to do well. This next approach is better for benchmarking code (particularly faster bits of code).\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport timeit\n\ntimeit.timeit('x = np.exp(3.)', setup = 'import numpy as np', number = 100)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.000101420097053051\n```\n\n\n:::\n\n```{.python .cell-code}\ncode = '''\nx = np.exp(3.)\n'''\n\ntimeit.timeit(code, setup = 'import numpy as np', number = 100)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n9.014364331960678e-05\n```\n\n\n:::\n:::\n\n\nThat reports the **total** time for the 100 replications.\n\nWe can run it from the command line. \n\n\n::: {.cell}\n\n```{.bash .cell-code}\npython -m timeit -s 'import numpy' -n 1000 'x = numpy.exp(3.)'\n```\n\n\n::: {.cell-output .cell-output-stdout}\n\n```\n1000 loops, best of 5: 694 nsec per loop\n```\n\n\n:::\n:::\n\n\n`timeit` ran the code 1000 times for 5 different repetitions, giving the **average** time for the 1000 samples for the best of the 5 repetitions.\n\n\n### Profiling\n\nThe `Cprofile` module will show you how much time is spent in\ndifferent functions, which can help you pinpoint bottlenecks in your\ncode.\n\nI haven't run this code when producing this document as the output of\nthe profiling can be lengthy.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef lr_slow(y, x):\n    xtx = x.T @ x\n    xty = x.T @ y\n    inv = np.linalg.inv(xtx)\n    return inv @ xty\n\n## generate random observations and random matrix of predictors\nn = 7000\ny = np.random.normal(size = 7000)\nx = np.random.normal(size = (7000,1000))\n\nt0 = time.time()\nregr = lr_slow(y, x)\nt1 = time.time()\nprint(f\"Execution time: {t1-t0} seconds.\")\n\nimport cProfile\ncProfile.run('lr_slow(y,x)')\n```\n:::\n\n\n\nThe `cumtime` column includes the time spent in nested calls to functions while the `tottime` column excludes it.\n\nAs we'll discuss in detail in Unit 10, we almost never want to explicitly invert\na matrix. Instead we factorize the matrix and use the factorized result to do the\ncomputation of interest. In this case using the Cholesky decomposition is a standard\napproach, followed by solving triangular systems of equations.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport scipy as sp\n\ndef lr_fast(y, x):\n    xtx = x.T @ x\n    xty = x.T @ y\n    L = sp.linalg.cholesky(xtx)\n    out = sp.linalg.solve_triangular(L.T, \n          sp.linalg.solve_triangular(L, xty, lower=True),\n          lower=False)\n    return out\n\nt0 = time.time()\nregr = lr_fast(y, x)\nt1 = time.time()\nprint(f\"Execution time: {t1-t0} seconds.\")\n\ncProfile.run('lr_fast(y,x)')\n```\n:::\n\n\nIn principle, the Cholesky now dominates the computational time (but is much faster than `inv`),\nso there's not much more we can do in this case. That said, it's not obvious from the profiling\nthat the fact that most of the time is in the Cholesky is the case. Interpreting the output of a \nprofiler can be hard. In this case to investigate further I would probably time individual steps\nwith `timeit`.\n\nYou might wonder if it's better to use `x.T` or `np.transpose(x)`. Try using `timeit` to decide.\n\nThe Python profilers (`cProfile` and `profile` (not shown)) use [deterministic profiling](https://docs.python.org/3/library/profile.html#what-is-deterministic-profiling) -- calculating the interval between events (i.e., function calls and returns). However, there is some limit to accuracy -- the underlying 'clock' measures in units of about 0.001 seconds. \n\n(In contrast, R's profiler works by sampling (statistical profiling) - every little while during a calculation it finds out what function R is in and saves that information to a file. So if you try to profile code that finishes really quickly, there's not enough opportunity for the sampling to represent the calculation accurately and you may get spurious results.)\n\n\n\n## Writing efficient (Python) code\n\nWe'll discuss a variety of these strategies, including:\n\n- Pre-allocating memory rather than growing objects iteratively\n- Vectorization and use of fast matrix algebra\n- Consideration of loops vs. map operations\n- Speed of lookup operations, including hashing\n\nWhile illustrated in Python, many of the underlying ideas pertain in other contexts.\n\n### Pre-allocating memory\n\nLet's consider whether we should pre-allocate space for the output of an operation or if it's ok to keep extending the length of an array or list.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nn = 100000\nz = np.random.normal(size = n)\n\n## Pre-allocation\n\ndef fun_prealloc(vals):\n   n = len(vals)\n   x = [0] * n\n   for i in range(n):\n       x[i] = np.exp(vals[i])\n   return x\n\n## Appending to a list\n\ndef fun_append(vals):\n   x = []\n   for i in range(n):\n       x.append(np.exp(vals[i]))\n   return x\n\n## Appending to a numpy array\n\ndef fun_append_np(vals):\n   x = np.array([])\n   for i in range(n):\n       x = np.append(x, np.exp(vals[i]))\n   return x\n\n\nt0 = time.time()\nout1 = fun_prealloc(z)\ntime.time() - t0\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.10391902923583984\n```\n\n\n:::\n\n```{.python .cell-code}\nt0 = time.time()\nout2 = fun_append(z)\ntime.time() - t0\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.1037147045135498\n```\n\n\n:::\n\n```{.python .cell-code}\nt0 = time.time()\nout3 = fun_append_np(z)\ntime.time() - t0\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n2.4576478004455566\n```\n\n\n:::\n:::\n\n\nSo what's going on? First let's consider what is happening with the use of `np.append`.\nNote that it is a function, rather than a method, and we need to reassign to `x`.\nWhat must be happening in terms of memory use and copying when we append an element?\n\n\n::: {.cell}\n\n```{.python .cell-code}\nx = np.random.normal(size = 5)\nid(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n134424012514064\n```\n\n\n:::\n\n```{.python .cell-code}\nid(np.append(x, 3.34))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n134424012513488\n```\n\n\n:::\n:::\n\n\nWe can avoid that large cost of copying and memory allocation by pre-allocating space for the entire output array. (This is equivalent to variable initialization in compiled languages.)\n\nOk, but how is it that we can append to the **list** at apparently no cost?\n\nIt's not magic, just that Python is clever. Let's get an idea of what is going on:\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef fun_append2(vals):\n   n = len(vals)\n   x = []\n   print(f\"Initial id: {id(x)}\")\n   sz = sys.getsizeof(x)\n   print(f\"iteration 0: size {sz}\")\n   for i in range(n):\n       x.append(np.exp(vals[i]))\n       if sys.getsizeof(x) != sz:\n           sz = sys.getsizeof(x)\n           print(f\"iteration {i}: size {sz}\")\n   print(f\"Final id: {id(x)}\")\n   return x\n\nz = np.random.normal(size = 1000)\nout = fun_append2(z)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nInitial id: 134424011871424\niteration 0: size 56\niteration 0: size 88\niteration 4: size 120\niteration 8: size 184\niteration 16: size 248\niteration 24: size 312\niteration 32: size 376\niteration 40: size 472\niteration 52: size 568\niteration 64: size 664\niteration 76: size 792\niteration 92: size 920\niteration 108: size 1080\niteration 128: size 1240\niteration 148: size 1432\niteration 172: size 1656\niteration 200: size 1912\niteration 232: size 2200\niteration 268: size 2520\niteration 308: size 2872\niteration 352: size 3256\niteration 400: size 3704\niteration 456: size 4216\niteration 520: size 4792\niteration 592: size 5432\niteration 672: size 6136\niteration 760: size 6936\niteration 860: size 7832\niteration 972: size 8856\nFinal id: 134424011871424\n```\n\n\n:::\n:::\n\n\nSurprisingly, the id of `x` doesn't seem to change, even though we are allocating new memory at many of the iterations. What is happening is that `x` is a wrapper object that contains within it a reference to an array of references (pointers) to the list elements. The location of the wrapper object doesn't change, but the underlying array of references/pointers is being reallocated. \n\nSide note: I don't know of any way to find out the location of the underlying array of pointers. I believe that under the hood, Python uses the `realloc` system call to request memory from the operating system when it needs to use additional pointers, and that the operating system can then try to allocate the additional memory at the same location (i.e., extending the block of memory).\n\nNote that as we discussed in the previous section on memory use, our assessment of size above does not include the actual size of the list elements, as illustrated by having one element of the list be a big object:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nprint(sys.getsizeof(out))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n8856\n```\n\n\n:::\n\n```{.python .cell-code}\nout[2] = np.random.normal(size = 100000)\nprint(sys.getsizeof(out))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n8856\n```\n\n\n:::\n:::\n\n\n:::{.callout-tip title=\"Grow objects using lists, then convert\"}\nOne upshot of how Python efficiently grows lists is that if you need to grow an object, use a Python list. Then once it is complete, you can always convert it to another type, such as a numpy array.\n:::\n\n### Vectorization and use of fast matrix algebra\n\nOne key way to write efficient Python code is to take advantage of numpy's\nvectorized operations.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nn = 10**6\nz = np.random.normal(size = n)\nt0 = time.time()\nx = np.exp(z)\nprint(time.time() - t0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.01285696029663086\n```\n\n\n:::\n\n```{.python .cell-code}\nx = np.zeros(n)  # Leave out pre-allocation timing to focus on computation.\nt0 = time.time()\nfor i in range(n):\n    x[i] = np.exp(z[i])\n\n\nprint(time.time() - t0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.7170143127441406\n```\n\n\n:::\n:::\n\n\nSo what is different in how Python handles the calculations above that\nexplains the huge disparity in efficiency? The vectorized calculation is being done natively\nin C in a for loop. The explicit Python for loop involves executing the for\nloop in Python with repeated calls to C code at each iteration. This involves a lot\nof overhead because of the repeated processing of the Python code inside the loop. For example,\nin each iteration of the loop, Python is checking the types of the variables because it's possible\nthat the types might change, as discussed earlier.\n\nYou can usually get a sense for how quickly a Python call will pass things along\nto C or Fortran by looking at the body of the relevant function(s) being called.\n\nUnfortunately seeing the source code in Python often involves going and finding it in a file on disk,\nwhereas in R, printing a function will show its source code. However you can use `??` in IPython\nto get the code for non-builtin functions. Consider `numpy.linspace??`.\n\nHere I found the source code for the scipy `triangular_solve` function, which calls out to a Fortran function `trtrs`, found in the LAPACK library.\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n## On an SCF machine:\n/usr/local/linux/miniforge-3.12/lib/python3.12/site-packages/scipy/linalg/_basic.py\n```\n:::\n\n\nWith a bit more digging around we could verify that `trtrs` is a LAPACK funcion by doing some grepping:\n\n```\n./linalg/_basic.py:    trtrs, = get_lapack_funcs(('trtrs',), (a1, b1))\n```\n\nMany numpy and scipy functions  allow you to pass in arrays, and operate on those\narrays in vectorized fashion. So before writing a for loop, look\nat the help information on the relevant function(s) to see if they\noperate in a vectorized fashion. Functions might take arrays for one or more of their arguments.\n\nOutside of the numerical packages, we often have to manually do the looping:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nx = [3.5, 2.7, 4.6]\ntry:\n    math.cos(x)\nexcept Exception as error:\n    print(error)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nmust be real number, not list\n```\n\n\n:::\n\n```{.python .cell-code}\n[math.cos(val) for val in x]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[-0.9364566872907963, -0.9040721420170612, -0.11215252693505487]\n```\n\n\n:::\n\n```{.python .cell-code}\nlist(map(math.cos, x))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[-0.9364566872907963, -0.9040721420170612, -0.11215252693505487]\n```\n\n\n:::\n:::\n\n\n:::{.callout-tip title=\"Challenge\"}\nConsider the chi-squared statistic involved in\na test of independence in a contingency table:\n\n$$\n\\chi^{2}=\\sum_{i}\\sum_{j}\\frac{(y_{ij}-e_{ij})^{2}}{e_{ij}},\\,\\,\\,\\, e_{ij}=\\frac{y_{i\\cdot}y_{\\cdot j}}{y_{\\cdot\\cdot}}\n$$\n\nwhere $y_{i\\cdot}=\\sum_{j}y_{ij}$ and $y_{\\cdot j} = \\sum_{i} y_{ij}$ and $y_{\\cdot\\cdot} = \\sum_{i} \\sum_{j} y_{ij}$. Write this in a vectorized way\nwithout any loops.  Note that 'vectorized' calculations also work\nwith matrices and arrays.\n:::\n\nSometimes we can exploit vectorized mathematical operations in\nsurprising ways, though sometimes the\ncode is uglier. \n\n\n::: {.cell}\n\n```{.python .cell-code}\nx = np.random.normal(size = n)\n\n## List comprehension\ntimeit.timeit('truncx = [max(0,val) for val in x]', number = 10, globals = {'x':x})\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n1.7045243841130286\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n## Vectorized slice replacement\ntimeit.timeit('truncx = x.copy(); truncx[x < 0] = 0', number = 10, globals = {'x':x})\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.06893204897642136\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n## Vectorized math trick\ntimeit.timeit('truncx = x * x>0', number = 10, globals = {'x':x})\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.015284791123121977\n```\n\n\n:::\n:::\n\n\nWe'll discuss what has to happen (in terms of calculations, memory allocation, and copying) in the two vectorized approaches to try to understand which is more efficient.\n\n:::{.callout-tip title=\"Additional efficiency tips\"}\n\n - If you do need to loop over dimensions of a matrix or array, if possible\nloop over the smallest dimension and use the vectorized calculation\non the larger dimension(s). For example if you have a 10000 by 10 matrix, try to set\nup your problem so you can loop over the 10 columns rather than the 10000 rows.\n - In general, in Python looping over rows is likely to be faster than looping over columns\nbecause of numpy's row-major ordering (by default, matrices are stored in memory as a long array in which values in a row are adjacent to each other). However how numpy handles this is [more complicated](https://numpy.org/doc/stable/dev/internals.html#multidimensional-array-indexing-order-issues) (see more in the [Section on cache-aware programming](#cache-aware-programming)), such that it may not matter for numpy calculations.\n - You can use direct arithmetic operations to add/subtract/multiply/divide\na 1-d array by each column of a matrix, e.g. `A*b` does element-wise multiplication of\neach row of *A* by a 1-d array *b*. If you need to operate\nby column, you can do it by transposing the matrix.\n::: \n\nCaution: relying on Python's broadcasting rule in the context of vectorized\noperations, such as is done when direct-multiplying a matrix by a\n1-d array to scale the columns relative to each other, can be dangerous as the code may not be easy for someone to read\nand poses greater dangers of bugs. In some cases you may want to\nfirst write the code more directly and\nthen compare the more efficient code to make sure the results are the same. It's also a good idea to  comment your code in such cases.\n\n### Vectorization, mapping, and loops\n\nNext let's consider when loops and mapping would be particularly slow and how mapping and loops might compare to each other.\n\nFirst, the potential for inefficiency of looping and map operations in interpreted languages will depend in part on whether a substantial part of the work is in the overhead involved in the looping or in the time required by the function evaluation on each of the elements.\n\nHere's an example, where the core computation is very fast, so we might expect the overhead of looping (in its various forms seen here) to be important. \n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport time\nn = 10**6\nx = np.random.normal(size = n)\n\nt0 = time.time()\nout = np.exp(x)\ntime.time() - t0\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.012586832046508789\n```\n\n\n:::\n\n```{.python .cell-code}\nt0 = time.time()\nvals = np.zeros(n)\nfor i in range(n):\n    vals[i] = np.exp(x[i])\n\n\ntime.time() - t0\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n1.186586618423462\n```\n\n\n:::\n\n```{.python .cell-code}\nt0 = time.time()\nvals = [np.exp(v) for v in x]\ntime.time() - t0\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.9369134902954102\n```\n\n\n:::\n\n```{.python .cell-code}\nt0 = time.time()\nvals = list(map(np.exp, x))\ntime.time() - t0\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.8540775775909424\n```\n\n\n:::\n:::\n\n\nRegardless of how we do the looping (an explicit loop, list comprehension, or `map`), it looks like we can't avoid the overhead unless we use the vectorized call,\nwhich is of course the recommended approach in this case, both for speed and readability (and conciseness).\n\nSecond, is it faster to use `map` than to use a loop? In the example above it is somewhat (but not substantially) faster to use `map` (and still much slower than vectorization). In the loop case, the interpreter needs to do the checking we discussed earlier in this section at each iteration of the loop. What about in the `map` case? For mapping over a numpy array, perhaps not, but what if mapping over a list? Without digging into how `map` works, it's hard to say, but it does seem that based on this example, `map` is not doing anything special that saves much time when mapping over the elements of a numpy array.\n\nHere's an example where the bulk of time is in the actual computation and not in the looping itself. We'll run a bunch of regressions on a matrix `X` (i.e., each column of `X` is a predictor) using each column of the matrix `mat` to do a separate regression. \n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport time\nimport statsmodels.api as sm\n\nn = 500000;\nnr = 10000\nnCalcs = int(n/nr)\n\nmat = np.random.normal(size = (nr, nCalcs))\n\nX = list(range(nr))\nX = sm.add_constant(X)\n\ndef regrFun(i):\n    model = sm.OLS(mat[:,i], X)\n    return model.fit().params[1]\n\nt0 = time.time()\nout1 = list(map(regrFun, range(nCalcs)))\ntime.time() - t0\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.08074760437011719\n```\n\n\n:::\n\n```{.python .cell-code}\nt0 = time.time()\nout2 = np.zeros(nCalcs)\nfor i in range(nCalcs):\n    out2[i] = regrFun(i)\n\n\ntime.time() - t0\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.05616569519042969\n```\n\n\n:::\n:::\n\n\nHere they're about the same time (depending on when I render the document, I am getting some stochasticity in the timing). This is not too surprising -- the overhead of the iteration should be small relative to the fundamental cost of the model fitting, and we wouldn't be concerned with the overhead of using a loop. \n\n### Matrix algebra efficiency\n\nOften calculations that are not explicitly linear algebra calculations\ncan be done as matrix algebra. If our Python installation has a fast (and possibly parallelized) BLAS, this allows our calculation to take advantage of it.\n\nFor example, we can sum the rows of a matrix by multiplying by a 1-d array of ones. \n\n\n::: {.cell}\n\n```{.python .cell-code}\nmat = np.random.normal(size=(500,500))\n\ntimeit.timeit('mat.dot(np.ones(500))', setup = 'import numpy as np',\n              number = 1000, globals = {'mat': mat})\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.030468731187283993\n```\n\n\n:::\n\n```{.python .cell-code}\ntimeit.timeit('np.sum(mat, axis = 1)', setup = 'import numpy as np',\n              number = 1000, globals = {'mat': mat})\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.11560096777975559\n```\n\n\n:::\n:::\n\n\nGiven the extra computation involved in actually multiplying each number by one, it's surprising that this is faster than numpy sum function. One thing we'd want to know is whether the BLAS matrix multiplication call is being done in parallel.\n\nOn the other hand, big matrix operations can be slow.\n\n:::{.callout-tip title=\"Challenge\"}\nSuppose you\nwant a new matrix that computes the differences between successive\ncolumns of a matrix of arbitrary size. How would you do this as matrix\nalgebra operations? It's possible to write it as multiplying the matrix\nby another matrix that contains 0s, 1s, and -1s in appropriate places.\n Here it turns out that the\n*for* loop is much faster than matrix multiplication. However,\nthere is a way to do it faster as matrix direct subtraction. \n:::\n\n### Order of operations and efficiency\n\nWhen doing matrix algebra, the order in which you do operations can\nbe critical for efficiency. How should I order the following calculation?\n\n\n::: {.cell}\n\n```{.python .cell-code}\nn = 5000\nA = np.random.normal(size=(n, n))\nB = np.random.normal(size=(n, n))\nx = np.random.normal(size=n)\n\nt0 = time.time()\nres1 = (A @ B) @ x\nprint(time.time() - t0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n2.171241044998169\n```\n\n\n:::\n\n```{.python .cell-code}\nt0 = time.time()\nres1 = A @ (B @ x)\nprint(time.time() - t0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.03216886520385742\n```\n\n\n:::\n:::\n\n\n:::{.callout-tip title=\"Challenge\"}\nWhy is the second order much faster?\n\nCount the number of multiplications involved in each of the two approaches.\n:::\n\n### Avoiding unnecessary operations \n\nWe can use the matrix direct product (i.e., `A*B`) to do\nsome manipulations much more quickly than using matrix multiplication.\n\n:::{.callout-tip title=\"Challenge\"}\nHow can I use the direct product to find the trace\nof a matrix, $XY$? \n:::\n\nWhen working with diagonal matrices, you can generally get much faster results by being smart. The following operations: $X+D$, $DX$, $XD$\nare mathematically the sum of two matrices and products of two matrices.\nBut we can do the computation without using two full matrices.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nn = 1000\nX = np.random.normal(size=(n, n))\ndiagvals = np.random.normal(size=n)\nD = np.diag(diagvals)\n\n# The following lines are very inefficient\nsummedMat = X + D\nprodMat1 = D @ X\nprodMat2 = X @ D\n```\n:::\n\n\n:::{.callout-tip title=\"Challenge\"}\nConsider either $X+D$, $DX$, or $XD$. How can I use vectorization to do this much more quickly than the direct, naive translation of the math into code?\n:::\n\n:::{.callout-important title=\"Exploit sparsity/structure!\"}\nMore generally, sparse matrices and structured matrices (such as block\ndiagonal matrices) can generally be worked with MUCH more efficiently\nthan treating them as arbitrary matrices. The `scipy.sparse` package (for both structured and arbitrary\nsparse matrices)  can help, as can specialized code available in other languages,\nsuch as C and Fortran packages.\n:::\n\n### Speed of lookup operations\n\nThere are lots of situations in which we need to retrieve values for subsequent computations. In some cases we might be retrieving elements of an array or looking up values in a dictionary.\n\nLet's compare the speed of some different approaches to lookup.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nn = 1000\nx = list(np.random.normal(size = n))\nlabels = [str(v) for v in range(n)]\nxD = dict(zip(labels, x))\n\ntimeit.timeit(\"x[500]\", number = 10**6, globals = {'x':x})\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.02141748834401369\n```\n\n\n:::\n\n```{.python .cell-code}\ntimeit.timeit(\"xD['500']\", number=10**6, globals = {'xD':xD})\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.03072414081543684\n```\n\n\n:::\n:::\n\n\nHow is it that Python can look up by key in the dictionary at essentially the same speed as jumping to an index position? It uses hashing, which allows O(1) lookup. In contrast, if one has to look through each label (key) in turn, that is O(n), which is much slower:\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntimeit.timeit(\"x[labels.index('500')]\", number = 10**6, globals = {'x':x, 'labels': labels})\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n6.690881405957043\n```\n\n\n:::\n:::\n\n\nAs a further point of contrast, if we look up elements by name in R in named vectors or lists, that is much slower than looking up by index, because R doesn't use hashing in that context and  has to scan through the objects one by one until it finds the one with the name it is looking for. This stands in contrast to R and Python being able to directly go to the position of interest based on the index of an array, or to the hash-based lookup in a Python dictionary or an R environment.\n\n### Hashing (including name lookup)\n\nAbove I mentioned that Python uses hashing to store and lookup values\nby key in a dictionary. \nI'll briefly describe what hashing is here, because it is a commonly-used\nstrategy in programming in general.\n\nA hash function is a function that takes as input some data (some input abitrary length) and maps it\nto a fixed-length output that can be used as a shortened reference to\nthe data. (The function should be deterministic, always returing the same\noutput for a given input.) We've seen this in the context of git commits where each\ncommit was labeled with a long base-16 number. This also comes up when\nverifying files on the Internet. You can compute the hash value on the\nfile you get and check that it is the same as the hash value associated\nwith the legitimate copy of the file. Even small changes in the file will result\nin a different hash value.\n\nWhile there are various uses of hashing, for our purposes here, hashing can allow one to look up values by their\nname via a hash table. The idea is that you have a set of key-value\npairs (sometimes called a dictionary) where the key is the name\nassociated with the value and the value is some arbitrary object.\nYou want to be able to quickly find the value/object quickly.\n\nHashing allows one to quickly determine an index associated with the key\nand therefore quickly find the relevant value based on the index. For\nexample, one approach is to compute the hash as a function of the key\nand then take the remainder when dividing by the number of possible results\n(here the fact that the result is a fixed-length output is important) to get the index.\nHere's the procedure in pseudocode:\n\n```\n    hash = hashfunc(key) \n    index = hash %% array_size \n    ## %% is modulo operator - it gives the remainder\n```\n\nIn general, there will be collisions -- multiple keys will be assigned to the\nsame index (this is unavoidable because of the fact that the hash function returns a fixed length output). However with a good hash function, usually there will be a small number of keys associated\nwith a given bucket. So each bucket will contain a list of a small number of values and the associated keys.\n(The buckets might contain the actual values or they might contain the addresses of where the values are actually stored\nif the values are complicated objects.) Then determining the correct value (or the required address) within\na given bucket is fast even with simple linear search through the items one by one.\nPut another way, the\nhash function distributes the keys amongst an array of buckets and\nallows one to look up the appropriate bucket quickly based on the\ncomputed index value. When the hash table is properly set up, the cost\nof looking up a value does not depend on the number of key-value pairs\nstored.\n\nPython uses hashing to look up the value  based on the key in a given dictionary, and similarly when looking up variables in namespaces. This allows Python to retrieve objects very quickly.\n\n\n## Additional general strategies for efficiency\n\nIt's also useful to be aware of some other strategies for improving efficiency.\n\n### Cache-aware programming\n\nIn addition to main memory (what we usually mean when we talk about RAM), computers also have memory caches, which are small amounts of fast memory that can be accessed very quickly by the processor. For example your computer might have L1, L2, and L3 caches, with L1 the smallest and fastest and L3 the largest and slowest. The idea is to try to have the data that is most used by the processor in the cache. \n\nIf the next piece of data needed for computation is available in the cache, this is a *cache hit* and the data can be accessed very quickly. However, if the data is not available in the cache, this is a *cache miss* and the speed of access will be a lot slower. *Cache-aware programming* involves writing your code to minimize cache misses. Generally when data is read from memory it will be read in chunks, so values that are contiguous will be read together.\n\nHow does this inform one's programming? For example, if you have a matrix of values stored in row-major order, computing on a row will be a lot faster than computing on a column, because the row can be read into the cache from main memory and then accessed in the cache. In contrast, if the matrix is large and therefore won't fit in the cache, when you access the values of a column, you'll have to go to main memory repeatedly to get the values for the row because the values are not stored contiguously.\n\nThere's a nice example of the importance of the cache at [the bottom of this blog post](https://wrathematics.github.io/2016/10/28/comparing-symmetric-eigenvalue-performance/).\n\nIf you know the size of the cache, you can try to design your code so that in a given part of your code you access data structures that will fit in the cache. This sort of thing is generally more relevant if you're coding in a language like C. But it can matter sometimes in interpreted languages too.\n\nLet's see what happens in Python. By default, matrices in numpy are row-major, also called \"C order\".\nI'll create a long matrix with a small number of very long columns and a wide matrix with a small number of very long rows.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nnr = 800000\nnc = 100\n\nA = np.random.normal(size=(nr, nc))   # long matrix\ntA = np.random.normal(size=(nc, nr))  # wide matrix\n\n## Verify that A is row-major using `.flags` (notice the `C_CONTIGUOUS: True`).\nA.flags\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  C_CONTIGUOUS : True\n  F_CONTIGUOUS : False\n  OWNDATA : True\n  WRITEABLE : True\n  ALIGNED : True\n  WRITEBACKIFCOPY : False\n```\n\n\n:::\n:::\n\n\nNote that I didn't use `A.T` or `np.transpose` as that doesn't make a copy in memory and so the transposed\nmatrix doesn't end up being row-major. You can use `A.flags` and A.T.flags` to see this.\n\nNow let's time calculating the sum by column in the long matrix vs. the sum by row in the wide matrix. Exactly the same number of arithmetic operations needs to be done in an equivalent manner for the two cases.\nWe want to use a large enough matrix so the entire matrix doesn't fit in the cache,\nbut not so large that the example takes a long time or a huge amount of memory.\nWe'll use a rectangular matrix, such that the summation for a single column of the long matrix\nor a single row of the wide matrix involves many numbers, but there are a limited number of such\nsummations. This focuses the example on the efficiency of the column-wise vs. row-wise summation\nrather than any issues that might be involved in managing large numbers of such summations (e.g.,\ndoing many, many summations that involve just a few numbers).\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Define the sum calculations as functions\ndef sum_by_row():\n    return np.sum(tA, axis=1)\n\ndef sum_by_column():\n    return np.sum(A, axis=0)\n\ntimeit.timeit(sum_by_row, number=10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.38696206267923117\n```\n\n\n:::\n\n```{.python .cell-code}\ntimeit.timeit(sum_by_column, number=10)  # potentially slow\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.46841978281736374\n```\n\n\n:::\n:::\n\n\nSuppose we instead do the looping manually.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntimeit.timeit('[np.sum(A[:,col]) for col in range(A.shape[1])]',\n    setup = 'import numpy as np', number=10, globals = {'A': A})   \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n5.087795170024037\n```\n\n\n:::\n\n```{.python .cell-code}\ntimeit.timeit('[np.sum(tA[row,:]) for row in range(tA.shape[0])]',\n    setup = 'import numpy as np', number=10, globals = {'tA': tA})  \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.3881711093708873\n```\n\n\n:::\n:::\n\n\nIndeed, the row-wise calculations are much faster when done manually. However, when done with the `axis` argument in `np.sum` there is little difference. So that suggests numpy might be doing something clever in its implementation of `sum` with the `axis` argument.\n\n:::{.callout-tip title=\"Challenge\"}\nSuppose you were writing code for this kind of use case. How could you set up your calculations to do either row-wise or column-wise operations in a way that processes each number sequentially based on the order in which the numbers are stored? For example suppose the values are stored row-major but you want the column sums.\n:::\n\nWhen we define a numpy array, we can choose to use column-major order (i.e., \"Fortran\" order) with the `order` argument.\n\n### Loop fusion\n\nLet's consider this (vectorized) code:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nx = np.exp(x) + 3*np.sin(x)\n```\n:::\n\n\nThis code has some downsides.\n\n  - Think about whether any additional memory has to be allocated.\n  - Think about how many for loops will have to get executed.\n\nContrast that to running directly as a for loop (e.g., here in Julia or in C/C++):\n\n\n::: {.cell}\n\n```{.julia .cell-code}\nfor i in 1:length(x)\n    x[i] = exp(x[i]) + 3*sin(x[i])\nend\n```\n:::\n\n\nHow does that affect the downsides mentioned above?\n\nCombining loops is called 'fusing' and is an [important optimization that Julia can do](https://docs.julialang.org/en/v1/manual/performance-tips/#More-dots:-Fuse-vectorized-operations), as shown in [this demo](https://computing.stat.berkeley.edu/tutorial-parallelization/parallel-julia#4-loops-and-fused-operations). It’s also a [key optimization done by XLA](https://www.tensorflow.org/xla), a compiler used with JAX and Tensorflow, so one approach to getting loop fusion in Python is to use JAX (or Tensorflow) for such calculations within Python rather than simply using numpy.\n\n### JIT compilation (with JAX)\n\nYou can think of JAX as a version of numpy enabled to use the GPU (or automatically parallelize on CPU threads) and provide automatic differentiation. \n\nWe can also JIT compile JAX code. Behind the scenes, the instructions are compiled to machine code for different backends (e.g., CPU and GPU) using the XLA compiler.\n\nLet's first consider running a vectorized calculation using JAX on the CPU, which will use multiple threads (discussed in Unit 6), each thread running on a separate CPU core on our computer. For now all we need to know is that the calculation will run in parallel, so we expect it to be faster than when using numpy, which won't run the calculation in parallel.\n\n:::{.callout-warning}\nThis demo uses 4 GB of memory just for `x`. Run on your own machine with caution.\n:::\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport time\nimport numpy as np\nimport jax.numpy as jnp\n\ndef myfun_np(x):\n    y = np.exp(x) + 3 * np.sin(x)\n    return y\n    \ndef myfun_jnp(x):\n    y = jnp.exp(x) + 3 * jnp.sin(x)\n    return y\n\nn = 500000000\n\nx = np.random.normal(size = n).astype(np.float32)  # 32-bit for consistency with JAX default\nx_jax = jnp.array(x)  # 32-bit by default\nprint(x_jax.platform())\n```\n:::\n\n\n```\ncpu\n```\n\n\n::: {.cell}\n\n```{.python .cell-code}\nt0 = time.time()\nz = myfun_np(x)\nt1 = time.time() - t0\n\nt0 = time.time()\nz_jax = myfun_jnp(x_jax).block_until_ready()\nt2 = time.time() - t0\n\nprint(f\"numpy time: {round(t1,3)}\\njax time: {round(t2,3)}\")\n```\n:::\n\n\nRunning on the SCF gandalf machine (not shown above), we get these times.\n\n```\nnumpy time: 17.181\njax time: 5.722\n```\n\nThere's a nice speedup compared to numpy.\n\nSince JAX will often execute computations asynchronously (in particular when using the GPU), the `block_until_ready` invocation ensures that the computation finishes before we stop timing.\n\nBy default the JAX floating point type is 32-bit so we forced the use of 32-bit numbers for numpy\nfor comparability. One could have JAX use 64-bit numbers like this:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport jax\njax.config.update(\"jax_enable_x64\", True)  \n```\n:::\n\n\nNext let's consider JIT compiling it, which should fuse the vectorized operations and avoid temporary objects. The JAX docs have a [nice discussion](https://jax.readthedocs.io/en/latest/notebooks/thinking_in_jax.html#to-jit-or-not-to-jit) of when JIT compilation will be beneficial.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport jax\nmyfun_jnp_jit = jax.jit(myfun_jnp)\n\nt0 = time.time()\nz_jax_jit = myfun_jnp_jit(x_jax).block_until_ready()\nt3 = time.time() - t0\nprint(f\"jitted jax time: {round(t3,3)}\")\n```\n:::\n\n\n```\njitted jax time: 3.218\n```\n\nSo that gives a nice two-fold additional speedup.\n\n\n### Lazy evaluation \n\nWhat's strange about this R code?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nf <- function(x) print(\"hi\")\nsystem.time(mean(rnorm(1000000)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n  0.056   0.004   0.061 \n```\n\n\n:::\n\n```{.r .cell-code}\nsystem.time(f(3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"hi\"\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n      0       0       0 \n```\n\n\n:::\n\n```{.r .cell-code}\nsystem.time(f(mean(rnorm(1000000)))) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"hi\"\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n  0.001   0.000   0.002 \n```\n\n\n:::\n:::\n\n\nIt seems like the `rnorm(1000000)` is not actually executed when being passed to `f`. That is \"lazy evaluation\" in action - the code that is passed in as an argument is only evaluated when it is needed (and in this case it's not needed for the function to run).\n\nLazy evaluation is not just an R thing. It also occurs in Tensorflow (particularly version 1),\nthe Python Dask package, and in Spark. The basic idea is to delay executation until\nit's really needed, with the goal that if one does so, the system may be\nable to better optimize a series of multiple steps as a joint operation\nrelative to executing them one by one.\n\nHowever, Python itself does not have lazy evaluation.\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}